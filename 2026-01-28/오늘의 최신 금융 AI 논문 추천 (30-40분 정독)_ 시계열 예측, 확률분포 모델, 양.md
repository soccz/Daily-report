<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# ì˜¤ëŠ˜ì˜ ìµœì‹  ê¸ˆìœµ AI ë…¼ë¬¸ ì¶”ì²œ (30-40ë¶„ ì •ë…): ì‹œê³„ì—´ ì˜ˆì¸¡, í™•ë¥ ë¶„í¬ ëª¨ë¸, ì–‘ì ê¸ˆìœµì— ì¤‘ì . ë…¼ë¬¸ëª… / ì €ì / í•œì¤„ í•µì‹¬ / ìˆ˜ì‹ 3-4ê°œ (ì˜ë¯¸ ì„¤ëª… í¬í•¨) / ê¸ˆìœµ ì‘ìš© (ì—…ë¹„íŠ¸/KOSPI/í™˜ìœ¨) / êµ¬í˜„ íŒ / ë‹¤ìŒ ë…¼ë¬¸ ì—°ê²°ê³ ë¦¬. ë¹ ë¥´ê³  ëª…í™•í•˜ê²Œ. ì•„ì¹¨ë‰´ìŠ¤ì²˜ëŸ¼.

ì´ì œ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.

***

# ğŸ“° ê¸ˆìœµ AI ìµœì‹  ë…¼ë¬¸ ì¡°ì°¬ ë¦¬í¬íŠ¸ (2026ë…„ 1ì›” 28ì¼)

## ğŸ¯ ì˜¤ëŠ˜ì˜ Top 3 í•µì‹¬ ë…¼ë¬¸


***

## **1ï¸âƒ£ Powerformer: ì¸ê³¼ì  ê°€ì¤‘ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜**[^1][^2]

**ì €ì**: Kareem Hegazy, Michael W. Mahoney, N. Benjamin Erichson (UC Berkeley, Lawrence Berkeley Lab)
**ë°œí–‰**: 2025ë…„ 2ì›” | **í˜„í™©**: 47ê°œ ë²¤ì¹˜ë§ˆí¬ ê³¼ì œ ì¤‘ SOTA ë‹¬ì„±

### í•œì¤„ í•µì‹¬

ì‹œê³„ì—´ì˜ **ì‹œê°„ ì¸ê³¼ì„±**(causality)ê³¼ **ê·¼ì ‘ì„±**(locality)ì„ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ì—¬, ëª¨ë“  í† í°ì„ í‰ë“±í•˜ê²Œ ë³´ëŠ” Transformerì˜ ì•½ì ì„ ê·¹ë³µí•œ ì•„í‚¤í…ì²˜.

### í•µì‹¬ ìˆ˜ì‹ 4ê°œ

** í‘œì¤€ ì£¼ì˜(Standard Attention Similarity Score)**[^3]
$\mathbf{S}_h = \frac{\mathbf{K}_h \mathbf{Q}_h^\top}{\sqrt{d_k}}$

- ê¸°ì¡´ Transformerì˜ ëª¨ë“  ì‹œê°„ ê°„ ìƒí˜¸ì‘ìš©ì„ ë™ë“±í•˜ê²Œ ì·¨ê¸‰ â†’ ê¸ˆìœµ ì‹œê³„ì—´ì— ë¶€ì í•©

** ì¸ê³¼ì„± ë§ˆìŠ¤í¬ (Causal Mask)**[^4]
$\mathbf{M}^{(\text{C})}_{i,j} = \begin{cases} -\infty & j > i \\ 0 & j \leq i \end{cases}$

- ë¯¸ë˜ ì •ë³´ ëˆ„ìˆ˜ ì°¨ë‹¨ â†’ ì˜¨ë¼ì¸ íŠ¸ë ˆì´ë”©ì— í•„ìˆ˜

** ê°ì‡  ë§ˆìŠ¤í¬ (Decaying Locality Mask)** - **Powerformerì˜ í•µì‹¬ í˜ì‹ **[^5]
$\mathbf{M}^{(\text{D})}_{i,j} = \begin{cases} 0 & j > i \\ f(\Delta t) & j \leq i \end{cases}$

ì—¬ê¸°ì„œ $f(\Delta t) \leq 0$ëŠ” ë‹¤ìŒ ì¤‘ ì„ íƒ:

- **Weight Power-Law**: $f(\Delta t) = -\alpha \log(\Delta t)$ â†’ ëŠë¦° ê°ì‡  (ì¥ê¸° ì¶”ì„¸)
- **Similarity Power-Law**: $f(\Delta t) = -(\Delta t)^\alpha$ â†’ ë¹ ë¥¸ ê°ì‡  (ë‹¨ê¸° ë³€ë™ì„±)
- **Butterworth Filter**: $f_n(z) = \frac{-1}{\sqrt{1+(z/t_c)^{2n}}}$ â†’ ê³„ë‹¨ í•¨ìˆ˜ í˜•íƒœ

** ê°€ì¤‘ ì¸ê³¼ ì£¼ì˜ (Weighted Causal MHA)**[^6]
$\mathbf{C}_h^{(\text{C,D})} = \text{Softmax}(\mathbf{S}_h + \mathbf{M}^{(\text{C})} + \mathbf{M}^{(\text{D})})$
$C_{i,j}^{(\text{C,D})} \propto \begin{cases} 0 & j > i \\ \exp[f(\Delta t)] & j \leq i \end{cases}$

ê²°ê³¼ì ìœ¼ë¡œ ê°€ê¹Œìš´ ê³¼ê±° ì‹œì ì€ ë†’ì€ ê°€ì¤‘ì¹˜, ë¨¼ ê³¼ê±°ëŠ” ì§€ìˆ˜ì ìœ¼ë¡œ ê°ì†Œ â†’ **Power-law ìê¸°ìƒê´€ê³¼ ì¼ì¹˜**.

### ê¸ˆìœµ ì‘ìš© (ì—…ë¹„íŠ¸/KOSPI/í™˜ìœ¨)

| ì‹œì¥ | ì ìš© ì‚¬ë¡€ | ê¸°ëŒ€ íš¨ê³¼ |
| :-- | :-- | :-- |
| **ì—…ë¹„íŠ¸ (1ì‹œê°„ ë´‰)** | BTC/ETH ê°€ê²© ì˜ˆì¸¡ (336 â†’ 96 ì‹œê°„) | ë³€ë™ì„± êµ°ì§‘í™”(volatility clustering) í¬ì°©, í•˜ë½ì¥ ì¡°ê¸° ê²½ê³  |
| **KOSPI ì£¼ê°€** | ì½”ìŠ¤ë‹¥ ìˆ˜ìµë¥  ë¶„í¬ ì˜ˆì¸¡ | ë‹¨ê¸° ëª¨ë©˜í…€ (1-5ì¼) + ì¥ê¸° ì¶”ì„¸ (20-60ì¼) ë™ì‹œ í•™ìŠµ |
| **USD/KRW í™˜ìœ¨** | ì¼ì¤‘(intraday) ë³€ë™ì„± ì˜ˆì¸¡ | ì¤‘ì•™ì€í–‰ ë°œí‘œ ì§í›„ ê¸‰ë“±ë½ í¬ì°© (ê¹”ë”í•œ ì¸ê³¼ì„±) |

### êµ¬í˜„ íŒ: PyTorch + KerasTuner

```python
# Powerformer ì…ë ¥ ì¤€ë¹„ (KOSPI)
import numpy as np

# 1. ì¼ë³„ KOSPI ìˆ˜ìµë¥  (252Ã—30 = 1ë…„Ã—30ì¼ ì°½)
returns = np.log(prices[1:] / prices[:-1])  # ë¡œê·¸ ìˆ˜ìµë¥ 

# 2. íŒ¨ì¹˜ ìƒì„± (Patch-based embedding)
patch_size, stride = 16, 8
X_patched = np.lib.stride_tricks.as_strided(
    returns, 
    shape=((len(returns)-patch_size)//stride, patch_size),
    strides=(stride*returns.itemsize, returns.itemsize)
)

# 3. Power-law decay Î± ì„ íƒ
alpha = 0.5  # KOSPIëŠ” ë¹ ë¥¸ ê°ì‡  (ë‹¨ê¸° ë¯¼ê°)
# BTC/USDëŠ” alpha=1.0 ê¶Œì¥ (ëŠë¦° ê°ì‡ , ì¥ê¸° ì¶”ì„¸)

# 4. KerasTunerë¡œ ìµœì í™”
# - ì‹œí€€ìŠ¤ ê¸¸ì´: 336 (60ê±°ë˜ì¼ â‰ˆ 3ê°œì›”)
# - ë°°ì¹˜ í¬ê¸°: 128
# - í•™ìŠµë¥ : 0.002 (Adam)
```

**ë‹¤ìŒ ë…¼ë¬¸ìœ¼ë¡œì˜ ì—°ê²°**: PowerformerëŠ” **ì‹œê°„ êµ¬ì¡°ë§Œ** í•™ìŠµ â†’ ë¶„í¬ ì˜ˆì¸¡ì´ í•„ìš”í•˜ë©´ 2ë²ˆ ë…¼ë¬¸(LSTM-SSTD)ê³¼ ê²°í•©.

***

## **2ï¸âƒ£ LSTM-SSTD: ì™œê³¡ëœ Student's-t ë¶„í¬ ì˜ˆì¸¡**[^7][^8][^9]

**ì €ì**: Jakub MichaÅ„kÃ³w (TripleSun, KrakÃ³w)
**ë°œí–‰**: 2025ë…„ 8ì›” | **í…ŒìŠ¤íŠ¸**: S\&P 500, BOVESPA, DAX, WIG, æ—¥çµŒ225, KOSPI (2000-2021)

### í•œì¤„ í•µì‹¬

ì‹ ê²½ë§ì´ **ê¸ˆìœµ ìˆ˜ìµë¥ ì˜ í™•ë¥ ë¶„í¬ ìì²´ë¥¼ ì§ì ‘ ì˜ˆì¸¡** â†’ ë‹¨ìˆœ ê°€ê²© ì˜ˆì¸¡(ì  ì¶”ì •)ì„ ë„˜ì–´ ìœ„í—˜ ê´€ë¦¬(VaR/ES)ì™€ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”ì— ì¦‰ì‹œ í™œìš© ê°€ëŠ¥.

### í•µì‹¬ ìˆ˜ì‹ 4ê°œ

** í™•ë¥ ì  í”„ë ˆì„ì›Œí¬ (Probabilistic Framework)**[^3]
$r_t = \mu(x_t) + \varepsilon_t$
$\varepsilon_t = \sigma(x_t) z_t, \quad (z_t | x_t) \sim \text{iid} \, D(\eta(x_t))$

ì—¬ê¸°ì„œ:

- $r_t$: ì¼ì¼ ë¡œê·¸ ìˆ˜ìµë¥ 
- $\mu(x_t)$: LSTMì´ í•™ìŠµí•  ì¡°ê±´ë¶€ í‰ê·  (ë³´í†µ 0ì— ê°€ê¹Œì›€)
- $\sigma(x_t)$: ì‹œê°„ ë³€ë™ ë³€ë™ì„± (ì¡°ê±´ë¶€ í‘œì¤€í¸ì°¨)
- $D$: í™•ë¥ ë¶„í¬ íƒ€ì… (Normal, Student's-t, Skewed Student's-t)
- $\eta(x_t)$: ë¶„í¬ì˜ ì‹œê°„ ë³€ë™ ë§¤ê°œë³€ìˆ˜

** 3ê°€ì§€ ë¶„í¬ ì •ì˜**[^4]

A) **ì •ê·œë¶„í¬** (ê°€ìš°ì‹œì•ˆ) - ê¸°ì¤€ì„ 
$f_N(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{z^2}{2}\right)$

B) **Student's t** - ë¬´ê±°ìš´ ê¼¬ë¦¬(heavy tails) í¬ì°©
$f_{St}(z; \nu) = \frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})} \left(1 + \frac{z^2}{\nu}\right)^{-\frac{\nu+1}{2}}$

- $\nu$: ììœ ë„(degrees of freedom) - LSTM ì¶œë ¥
- $\nu \to \infty$ì´ë©´ ì •ê·œë¶„í¬ë¡œ ìˆ˜ë ´
- $\nu \approx 3-5$ì´ë©´ ê¸ˆìœµ ìˆ˜ìµë¥ ì˜ ê·¹ë‹¨ ê°’(tails) ì˜ ì„¤ëª…

C) **Skewed Student's t** - ì™œë„(skewness) + ë¬´ê±°ìš´ ê¼¬ë¦¬ ë™ì‹œ í¬ì°© â­
$f_{sSt}(x|\xi) = \frac{2}{\xi + \xi^{-1}} \left[f_{St}(\xi x)H_-(-x) + f_{St}(\xi^{-1} x)H_+(x)\right]$

- $\xi > 1$: ì˜¤ë¥¸ìª½ ì™œë„ (ì–‘ì˜ ì ë¦¼, ìƒìŠ¹ì¥)
- $\xi < 1$: ì™¼ìª½ ì™œë„ (ìŒì˜ ì ë¦¼, í•˜ë½ì¥)
- $H(\cdot)$: Heaviside í•¨ìˆ˜ (0 ë˜ëŠ” 1)
- **KOSPI/ì—…ë¹„íŠ¸ëŠ” í•­ìƒ $\xi < 1$** (ì¢Œì¸¡ ê¼¬ë¦¬ ë‘êº¼ì›€ = ê¸‰ë½ ìœ„í—˜)

** ì†ì‹¤í•¨ìˆ˜: ìŒì˜ ë¡œê·¸ìš°ë„ (Negative Log-Likelihood)**[^5]

ê° ë¶„í¬ë³„ë¡œ ë³„ë„ NLL ì •ì˜:
$\text{NLL}(\omega) = -\sum_{t=1}^n \ln f_D(r_t; \omega)$

ì˜ˆ) **Skewed Student's t NLL** (ê°€ì¥ ë³µì¡, ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥):
$\text{NLL} = -\sum_{t=1}^n \ln\left(\frac{2}{\xi_t + \xi_t^{-1}}\right) + \sum_{t=1}^n \ln\sigma_t$
$- \sum_{t=1}^n \ln\left[f_{St}(\xi_t z_t; 0,1,\nu_t)H_-(-z_t) + f_{St}(\xi_t^{-1}z_t; 0,1,\nu_t)H_+(z_t)\right]$

ì—¬ê¸°ì„œ $z_t = (r_t - \mu_t)/\sigma_t$ (í‘œì¤€í™”ëœ ìˆ˜ìµë¥ ).

** ê°€ì¹˜-ìœ„í—˜ (Value-at-Risk) ê³„ì‚°**[^6]

ì˜ˆì¸¡ëœ ë¶„í¬ë¡œë¶€í„° VaR(1%) ì¶”ì •:
$\text{VaR}_{t+1}(\alpha) = -\mu_{t+1} - \sigma_{t+1} \cdot q_{z,\alpha}$

- $q_{z,\alpha}$: í•™ìŠµëœ ë¶„í¬ì˜ $\alpha$-ë¶„ìœ„ìˆ˜
- KOSPI ì˜ˆ: VaR(1%) = -3.2% (1% í™•ë¥ ë¡œ ì¼ì¼ ìˆ˜ìµë¥  -3.2% ì´í•˜)

ê¸°ëŒ“ê°’ ë¶€ì¡±(Expected Shortfall):
$\text{ES}_{t+1}(\alpha) = E[r_{t+1} | r_{t+1} < \text{VaR}_{t+1}(\alpha)]$

- VaRë¥¼ ì´ˆê³¼í•  ë•Œì˜ **í‰ê·  ì†ì‹¤** â†’ ê·¹ë‹¨ ìœ„í—˜ ì •ëŸ‰í™”


### ê¸ˆìœµ ì‘ìš© (KOSPI/ì—…ë¹„íŠ¸/í™˜ìœ¨)

**ğŸ‡°ğŸ‡· KOSPI (ì½”ìŠ¤í”¼ ì§€ìˆ˜)**

- **LSTM-SSTD ì„±ê³¼**: VaR(1%) = 0.84% âœ… (ì´ë¡ ê°’ 1% ê¸°ì¤€)
    - ì¼ì¼ ë³€ë™ì„±: Ïƒ â‰ˆ 1.8-2.2%
    - ì˜ˆì¸¡ ê¸°ê°„: 2000-2021 (ê¸ˆìœµìœ„ê¸°, ìœ ë¡œí™” ìœ„ê¸°, ì½”ë¡œë‚˜ í¬í•¨)
    - ëª¨ë¸ì´ ê·¹ë‹¨ ì‚¬ê±´(2008 í­ë½, 2020 ê¸‰ë“±) í¬ì°©

**ğŸ’° ì—…ë¹„íŠ¸ (ë¹„íŠ¸ì½”ì¸/ì´ë”ë¦¬ì›€)**

- BTC/USDT ì¼ì¼ ìˆ˜ìµë¥ 
    - ê¼¬ë¦¬: $\nu \approx 2.5$ (ë§¤ìš° ë¬´ê±°ì›€ â†’ ë³€ë™ì„± ë§¤ìš° í¼)
    - ì™œë„: $\xi \approx 0.85$ (ì¢Œì¸¡ ê¼¬ë¦¬ ë‘êº¼ì›€ â†’ ê°‘ì‘ìŠ¤ëŸ° í•˜ë½ ìœ„í—˜)
    - **ì ìš©**: ë ˆë²„ë¦¬ì§€ ê±°ë˜ ì†ì‹¤ í•œë„(stop-loss) ì„¤ì •

```
Stop Loss = -Î¼ - Ïƒ Ã— q(1%)  # Skewed St-tì˜ 1% ë¶„ìœ„ìˆ˜
```


**ğŸ’± USD/KRW í™˜ìœ¨**

- í™˜ìœ¨ì€ ìƒëŒ€ì ìœ¼ë¡œ ì •ê·œë¶„í¬ì— ê°€ê¹Œì›€ ($\nu \approx 6-8$)
- ì •ì±… ë‰´ìŠ¤(ê¸ˆë¦¬ ê²°ì • ì§í›„)ì— ì™œë„ ì¦ê°€ ($\xi$ ë³€ë™)
- **í¬íŠ¸í´ë¦¬ì˜¤ í—¤ì§•**: í™˜ìœ¨ VaRë¥¼ ì£¼ì‹ VaRê³¼ í•¨ê»˜ ê³„ì‚°


### êµ¬í˜„ íŒ: Keras + TensorFlow

```python
import tensorflow as tf
import numpy as np
from scipy.special import gamma

# 1. KOSPI ì¼ì¼ ìˆ˜ìµë¥  ë¡œë“œ
# returns.shape = (2487, 1)  # 2000-2021, ì¼ì¼

# 2. 3-ì¸µ LSTM êµ¬ì„± (MichaÅ„kÃ³w ë…¼ë¬¸ ì •í™•)
model = tf.keras.Sequential([
    tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(10, 1)),
    tf.keras.layers.LSTM(64, return_sequences=True),
    tf.keras.layers.LSTM(32),
    # 3. Skewed Student's t ë¶„í¬: 4ê°œ ë§¤ê°œë³€ìˆ˜ ì¶œë ¥
    # [Î¼, Ïƒ, Î½, Î¾]
    tf.keras.layers.Dense(4, activation='linear')
])

# 3. Skewed Student's t NLL ì†ì‹¤í•¨ìˆ˜ (ìì²´ êµ¬í˜„)
def nll_skewed_student_t(y_true, y_pred):
    mu, sigma, nu, xi = tf.unstack(y_pred, axis=-1)
    
    # Ïƒ, Î½, Î¾ > 0 ê°•ì œ
    sigma = tf.nn.softplus(sigma) + 0.01
    nu = tf.nn.softplus(nu) + 2.0
    xi = tf.nn.softplus(xi) + 0.01
    
    # í‘œì¤€í™”
    z = (y_true[:, 0] - mu) / sigma
    
    # Heaviside í•¨ìˆ˜
    H_plus = tf.where(z >= 0, 1.0, 0.0)
    H_minus = tf.where(z < 0, 1.0, 0.0)
    
    # f_St: Student's t PDF (ë³µì¡í•œ ì‹, scipy ì°¸ê³ )
    # ê°„ë‹¨íˆ: ì§ì ‘ êµ¬í˜„ ë˜ëŠ” TFP(TensorFlow Probability) ì‚¬ìš©
    
    # NLL = -ln(2/(Î¾+Î¾^-1)) + ln(Ïƒ) - ln[...]
    nll = -tf.math.log(2 / (xi + 1/xi)) + tf.math.log(sigma)
    # ... (f_St ê³„ì‚° í›„ ì¶”ê°€)
    
    return tf.reduce_mean(nll)

# 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° (ë…¼ë¬¸ ê¸°ì¤€)
optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)
model.compile(optimizer=optimizer, loss=nll_skewed_student_t)

# 5. Walk-forward ê²€ì¦ (ì˜¨ë¼ì¸ í•™ìŠµ)
train_size = 252  # 1ë…„ (ê±°ë˜ì¼)
for t in range(train_size, len(returns)-96):
    X_train = returns[t-252:t].reshape(-1, 10, 1)
    y_train = returns[t-252+10:t]
    
    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)
    
    # ì˜ˆì¸¡
    X_test = returns[t:t+10].reshape(1, 10, 1)
    pred = model.predict(X_test)[^0]  # [Î¼, Ïƒ, Î½, Î¾]
    
    # VaR(1%) ê³„ì‚°
    mu_pred, sigma_pred, nu_pred, xi_pred = pred
    # ... Student's t CDFì˜ 1% ë¶„ìœ„ìˆ˜ ê³„ì‚°
    VaR_1pct = -mu_pred - sigma_pred * q_z_1pct(nu_pred, xi_pred)
```

**í‰ê°€ ì§€í‘œ:**

- **LPS** (Log Predictive Score): 1.19 (S\&P500, LSTM-SSTD) â†’ ìš°ìˆ˜
- **CRPS** (Continuous Ranked Probability Score): 0.51 â†’ ë¶„í¬ ì ì¤‘ë¥  ì¢‹ìŒ
- **PIT p-value**: 0.031 â†’ 95% ì‹ ë¢°ë„ë¡œ êµì •ë¨ (ëª¨ë¸ì˜ ë¶„í¬ ì¶”ì •ì´ ì •í™•)


### ë‹¤ìŒ ë…¼ë¬¸ìœ¼ë¡œì˜ ì—°ê²°

LSTM-SSTDëŠ” **ë‹¨ì¼ ìì‚°** VaR ê³„ì‚° â†’ ë‹¤ì¤‘ ìì‚° ìƒê´€ê³„ìˆ˜ê°€ í•„ìš”í•˜ë©´ 5ë²ˆ(THGNN)ë¡œ ì§„í–‰.

***

## **3ï¸âƒ£ T-KAN: ê³ ì£¼íŒŒ ê±°ë˜ë¥¼ ìœ„í•œ ì‹œê°„ Kolmogorov-Arnold ë„¤íŠ¸ì›Œí¬**[^10][^11][^12]

**ì €ì**: Ahmad Makinde
**ë°œí–‰**: 2026ë…„ 1ì›” | **í…ŒìŠ¤íŠ¸**: FI-2010 ì£¼ë¬¸ì¥(LOB) ë°ì´í„°ì…‹

### í•œì¤„ í•µì‹¬

LSTMì˜ ê³ ì •ëœ ì„ í˜• ê°€ì¤‘ì¹˜ë¥¼ **í•™ìŠµ ê°€ëŠ¥í•œ B-ìŠ¤í”Œë¼ì¸ í™œì„±í™” í•¨ìˆ˜**ë¡œ êµì²´ â†’ ì£¼ë¬¸ì¥ì˜ ë³µì¡í•œ ë¹„ì„ í˜• êµ¬ì¡° í¬ì°©, ì•ŒíŒŒ ê°ì‡  ê·¹ë³µ.

### í•µì‹¬ ìˆ˜ì‹ 3ê°œ + 1ê°œ (ì„±ê³¼)

** í‘œì¤€ LSTMì˜ ì•½ì **[^3]
$h_t = \tanh(W_h x_t + U_h h_{t-1} + b_h)$

- $W_h$, $U_h$: **ê³ ì •ëœ ì„ í˜• ê°€ì¤‘ì¹˜ í–‰ë ¬**
- ë¹„ì„ í˜• í™œì„±í™”($\tanh$)ëŠ” ìˆì§€ë§Œ, ì¤‘ê°„ ê³„ì‚°ì€ ì—¬ì „íˆ ì„ í˜• ë³€í™˜
- ì£¼ë¬¸ì¥: ìˆ˜ëŸ‰-ê°€ê²© ê´€ê³„ê°€ ë§¤ìš° ë¹„ì„ í˜• â†’ **LSTM ì„±ëŠ¥ í•œê³„**

** Kolmogorov-Arnold Network (KAN) í˜ì‹ ** â­[^4]
$y = \sum_{i=1}^n c_i \phi_i(x)$
ì—¬ê¸°ì„œ $\phi_i$ëŠ” **B-ìŠ¤í”Œë¼ì¸ ê¸°ì €í•¨ìˆ˜**:
$\phi_i(x) = \text{B-spline}(x; \text{knots}, \text{degree})$

- ê° ê°€ì¤‘ì¹˜ ëŒ€ì‹  **í•¨ìˆ˜(spline) í•™ìŠµ**
- ë¹„ì„ í˜•ë„ í•™ìŠµ â†’ ì‹ í˜¸ì˜ 'ëª¨ì–‘'ì„ ë°ì´í„°ë¡œë¶€í„° ì§ì ‘ ìŠµë“

** T-KAN ì•„í‚¤í…ì²˜**[^5]
$\text{T-KAN}(x) = \text{LSTM}_{\text{cells}} \text{ with KAN layers instead of linear } W$

- LSTMì˜ 3ê°œ ê²Œì´íŠ¸(input, forget, output)ì˜ ì„ í˜• ë³€í™˜ì„ KANìœ¼ë¡œ ëŒ€ì²´
- íŒŒë¼ë¯¸í„° ìˆ˜: 532,675 (DeepLOB 104,451 ëŒ€ë¹„ 5ë°°)
- **ë†’ì€ "ì´ìœ¤ ë°€ë„"(profitability per parameter)** â† ë§ì€ íŒŒë¼ë¯¸í„°ë„ ì •ë‹¹í™”

** ì„±ê³¼ (FI-2010 ì£¼ë¬¸ì¥ ì˜ˆì¸¡)**[^6]


| ëª¨ë¸ | F1-Score@k=100 | ìˆ˜ìµë¥ (1.0bps ê±°ë˜ ë¹„ìš©) | ë“œë¡œìš°ë‹¤ìš´ |
| :-- | :-- | :-- | :-- |
| **DeepLOB** (ê¸°ì¤€ì„ ) | ë‚®ìŒ | **-82.76%** | ì‹¬ê° |
| **T-KAN** (ì œì•ˆ) | +19.1% ê°œì„  | **+132.48%** | ìš°ìˆ˜ |

**í•´ì„**:

- DeepLOBëŠ” **í†µê³„ì  ì •í™•ë„(F1)**ì€ ìˆì§€ë§Œ **ê±°ë˜ ìˆ˜ìˆ˜ë£Œ ê·¹ë³µ ì‹¤íŒ¨**
- T-KANì€ ë†’ì€ ì‹ ë¢°ë„ì˜ ì‹œê·¸ë„ë§Œ ê±°ë˜ â†’ ìˆ˜ìµì„± ë‹¬ì„±


### ê¸ˆìœµ ì‘ìš© (ë§ˆì´í¬ë¡œ êµ¬ì¡°: ì—…ë¹„íŠ¸/ì„ ë¬¼)

**ğŸ” ë¹„íŠ¸ì½”ì¸ ì„ ë¬¼ (ì—…ë¹„íŠ¸, Binance Futures)**

- **ì…ë ¥**: í˜¸ê°€ ë°ì´í„° (ìµœìƒìœ„ 10ê°œ bid/ask ê°€ê²© \& ìˆ˜ëŸ‰)
- **ì˜ˆì¸¡**: ë‹¤ìŒ 100ms ê°€ê²© ë°©í–¥ (up/down/hold)
- **ì ìš©**:
    - Scalping: ì´ˆë‹¨ê¸° ìˆ˜ìµ í¬ì°©
    - Arbitrage: í˜„ë¬¼-ì„ ë¬¼ ê°„ ê°€ê²© ì°¨ìµ
    - Market making: ìµœì  bid-ask ìŠ¤í”„ë ˆë“œ ì„¤ì •

**ğŸ“Š KOSPI ì„ ë¬¼**

- ì£¼ë¬¸ì¥ ì •ë³´ë¥¼ í†µí•œ 10ì´ˆ ë’¤ ìˆ˜ìµë¥  ì˜ˆì¸¡
- ê¸°ê´€ íˆ¬ìì í¬ì§€ì…”ë‹ ê°ì§€ â†’ í›„í–‰ ê±°ë˜(momentum trading)


### êµ¬í˜„ íŒ: PyTorch + KAN Library

```python
import torch
import torch.nn as nn
from kan import KAN  # pip install pykan

# 1. ì£¼ë¬¸ì¥ ë°ì´í„° ì¤€ë¹„
# LOB: (ë°°ì¹˜, ì‹œê°„, 10 levels Ã— 4 features)
#   4 features = [ask_price, ask_qty, bid_price, bid_qty]
LOB_data = torch.randn(batch_size, 100, 40)  # 100 timestep, 40 ì°¨ì›

# 2. T-KAN êµ¬ì„±
class TKAN(nn.Module):
    def __init__(self, input_dim=40, hidden_dim=64, output_dim=3):
        super().__init__()
        
        # KAN ê¸°ë°˜ LSTM ì…€ (ìˆ˜ë™ êµ¬í˜„ ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ìš©)
        self.lstm_input = KAN([input_dim, hidden_dim])
        self.lstm_forget = KAN([input_dim, hidden_dim])
        self.lstm_output = KAN([input_dim, hidden_dim])
        self.lstm_cell = KAN([input_dim, hidden_dim])
        
        # ë¶„ë¥˜ í—¤ë“œ: 3ê°œ í´ë˜ìŠ¤ (up, neutral, down)
        self.fc = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        batch_size, seq_len, _ = x.shape
        h_t = torch.zeros(batch_size, hidden_dim)
        c_t = torch.zeros(batch_size, hidden_dim)
        
        for t in range(seq_len):
            x_t = x[:, t, :]
            
            # KAN ê¸°ë°˜ LSTM ê²Œì´íŠ¸ (B-spline ë¹„ì„ í˜•)
            i_t = torch.sigmoid(self.lstm_input(x_t))  # Input gate
            f_t = torch.sigmoid(self.lstm_forget(x_t))  # Forget gate
            o_t = torch.sigmoid(self.lstm_output(x_t))  # Output gate
            c_tilde = torch.tanh(self.lstm_cell(x_t))   # Cell candidate
            
            c_t = f_t * c_t + i_t * c_tilde
            h_t = o_t * torch.tanh(c_t)
        
        logits = self.fc(h_t)
        return logits

# 3. í•™ìŠµ
model = TKAN()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# 4. ê±°ë˜ ì‹œë®¬ë ˆì´ì…˜ (ë°±í…ŒìŠ¤íŒ…)
def backtest(model, lob_data, labels, transaction_cost=1.0):
    """ê±°ë˜ ë¹„ìš©(bps) í¬í•¨ ìˆ˜ìµë¥  ê³„ì‚°"""
    predictions = model(lob_data)  # shape: (n, 3)
    actions = torch.argmax(predictions, dim=1)  # (0=down, 1=hold, 2=up)
    
    returns = []
    for t in range(len(labels)-1):
        if actions[t] == 2:  # Predict UP
            actual_return = labels[t+1] - labels[t]  # ë‹¤ìŒ ë´‰ì˜ ìˆ˜ìµë¥ 
            net_return = actual_return - transaction_cost / 10000  # bps ë³€í™˜
            returns.append(net_return)
    
    total_return = np.sum(returns)  # T-KAN: +132.48% ëª©í‘œ
    return total_return
```

**ì„±ê³¼ ê¸°ì¤€:**

- **F1-Score@k=100**: > 55% (ì§§ì€ ì˜ˆì¸¡ ìœˆë„ìš°ì—ì„œ ì˜ë¯¸ ìˆìŒ)
- **Sharpe Ratio**: > 2.0 (ê³ ìˆ˜ìµ-ì €ìœ„í—˜ ê±°ë˜)
- **ìˆ˜ìµë¥ **: +100% ì´ìƒ (1.0bps ê±°ë˜ ë¹„ìš© í¬í•¨)


### ë‹¤ìŒ ë…¼ë¬¸ìœ¼ë¡œì˜ ì—°ê²°

T-KANì€ **ë§ˆì´í¬ë¡œ êµ¬ì¡°(í˜¸ê°€) ì˜ˆì¸¡** â†’ í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ì¤€ ìµœì í™”ëŠ” 5ë²ˆ(THGNN)ìœ¼ë¡œ í†µí•© ê°€ëŠ¥.

***

## ğŸ“Š ì¶”ê°€ ì£¼ëª© ë…¼ë¬¸ 2ê°œ

### **4ï¸âƒ£ EMAT: ë©€í‹° ì–´ìŠ¤í™íŠ¸ ì£¼ì˜ íŠ¸ëœìŠ¤í¬ë¨¸**[^13]

**ì €ì**: ë¯¸í™•ì¸ | **ë°œí–‰**: 2025ë…„ 9ì›” | **ì‹¤í—˜**: ì¤‘êµ­ SSE Index

**í˜ì‹ **: ë‹¨ìˆœ ì‹œê°„ ì˜ì¡´ì„±ë§Œ ì•„ë‹ˆë¼, **ì‹œê°„ ê°ì‡  + ì¶”ì„¸ + ë³€ë™ì„±**ì„ ë™ì‹œì— ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì— í¬í•¨.

**í•µì‹¬ ìˆ˜ì‹**:
$w_t = \exp(-\lambda \cdot |t - t_0|)$
ì—¬ê¸°ì„œ $\lambda$ëŠ” ì‹œê°„ ê°ì‡  ìƒìˆ˜ â†’ ìµœê·¼ ë°ì´í„°ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜.

**ì„±ê³¼**: SSE Composite RÂ² = 0.9591 (ë§¤ìš° ë†’ìŒ)

**ì‘ìš©**: KOSPI ì¼ì¼ ì¢…ê°€ ì˜ˆì¸¡, íŠ¹íˆ ì¶”ì„¸ ë°˜ì „ êµ¬ê°„ì—ì„œ ìš°ìˆ˜.

***

### **5ï¸âƒ£ THGNN: í•˜ì´ë¸Œë¦¬ë“œ íŠ¸ëœìŠ¤í¬ë¨¸-ê·¸ë˜í”„ ì‹ ê²½ë§**[^14][^15]

**ì €ì**: Jack Fanshawe, Rumi Masih, Alexander Cameron
**ë°œí–‰**: 2026ë…„ 1ì›” | **ì‹¤í—˜**: S\&P 500 ìƒê´€ê³„ìˆ˜ ì˜ˆì¸¡

**í˜ì‹ **: Transformer(ì‹œê°„) + Graph Attention(ìì‚° ê°„ ê´€ê³„) ê²°í•© â†’ 10ì¼ ë’¤ ì£¼ì‹ ê°„ ìƒê´€ê³„ìˆ˜ ì˜ˆì¸¡.

**ì…ë ¥**:

- ì¼ì¼ ìˆ˜ìµë¥  (ì‹œê³„ì—´)
- ê¸°ìˆ  ì§€í‘œ (RSI, MACD)
- ì„¹í„° ì •ë³´ (ì—ë„ˆì§€, IT, í—¬ìŠ¤)
- ë§¤í¬ë¡œ ì‹ í˜¸ (VIX, ìˆ˜ìµë¥ ê³¡ì„ )

**ì¶œë ¥**: S\&P 500 ì „ì²´ ìƒê´€ê³„ìˆ˜ í–‰ë ¬ (500Ã—500) 10ì¼ ë’¤ ì˜ˆì¸¡

**ì‘ìš©**:

1. **í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”**: Markowitz ë¶„ì‚° ìµœì†Œí™”
2. **ë°”ìŠ¤ì¼“ ê±°ë˜**: ìƒê´€ê³„ìˆ˜ ê¸°ë°˜ ë™ì  í´ëŸ¬ìŠ¤í„°ë§
3. **ë¦¬ìŠ¤í¬ ê´€ë¦¬**: ì½”ë¡œë‚˜ ì‹œê¸° ìƒê´€ê³„ìˆ˜ ê¸‰ì¦ ì˜ˆì¸¡

**ì„±ê³¼**: ê¸°ì¡´ rolling-window ëŒ€ë¹„ ìƒê´€ê³„ìˆ˜ ì˜¤ë¥˜ -35% ê°ì†Œ.

***

## ğŸ”— ë‹¤ìŒ ë…¼ë¬¸ ì—°ê²° í”Œë¡œìš°

```
1. Powerformer (ì‹œê°„ êµ¬ì¡° í•™ìŠµ)
   â†“
2. LSTM-SSTD (ë¶„í¬ ì˜ˆì¸¡: Î¼, Ïƒ, Î½, Î¾)
   â†“
3. THGNN (í¬íŠ¸í´ë¦¬ì˜¤ ìƒê´€ê³„ìˆ˜)
   â†“
4. T-KAN (ë§ˆì´í¬ë¡œ ê±°ë˜)
   â†“
5. ìµœì¢…: í†µí•© ì‹œìŠ¤í…œ (ë§¤í¬ë¡œ + ë§ˆì´í¬ë¡œ)
```


***

## ğŸ’» ì‹¤ì „ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

| í•­ëª© | ì¶”ì²œ | ë‚œì´ë„ | ì˜ˆìƒ ì‹œê°„ |
| :-- | :-- | :-- | :-- |
| Powerformer (ì½”ë“œ ìˆìŒ) | â­â­â­â­â­ | ì¤‘ | 1ì£¼ |
| LSTM-SSTD ì†ì‹¤í•¨ìˆ˜ | â­â­â­â­ | ì¤‘ìƒ | 2ì£¼ |
| THGNN êµ¬í˜„ | â­â­â­ | ìƒ | 3ì£¼ |
| T-KAN (KAN ë¼ì´ë¸ŒëŸ¬ë¦¬) | â­â­â­â­ | ìƒ | 2ì£¼ |
| ì „ì²´ ë°±í…ŒìŠ¤íŒ… ì‹œìŠ¤í…œ | â­â­ | ìƒìƒ | 4ì£¼+ |


***

## ğŸ“Œ ìš”ì  ì •ë¦¬ (5ë¶„ ìš”ì•½)

1. **Powerformer**: ì‹œê°„ ì¸ê³¼ì„± + ê·¼ì ‘ì„± â†’ ì‹œê³„ì—´ ê¸°ì´ˆ ëª¨ë¸
2. **LSTM-SSTD**: í™•ë¥ ë¶„í¬ ì˜ˆì¸¡ â†’ VaR/ES ì •ëŸ‰í™” (ë¦¬ìŠ¤í¬ ê´€ë¦¬)
3. **T-KAN**: ë§ˆì´í¬ë¡œ êµ¬ì¡° ì˜ˆì¸¡ â†’ ê³ ì£¼íŒŒ ê±°ë˜
4. **EMAT**: ë©€í‹° ì–´ìŠ¤í™íŠ¸ ì£¼ì˜ â†’ ì¶”ì„¸ + ë³€ë™ì„± ë™ì‹œ í•™ìŠµ
5. **THGNN**: í¬íŠ¸í´ë¦¬ì˜¤ ìƒê´€ê³„ìˆ˜ â†’ ë™ì  ë¦¬ë°¸ëŸ°ì‹±

***

## ğŸ“š ì½”ë“œ ì €ì¥ì†Œ (ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥)

- **Powerformer**: https://github.com/khegazy/Powerformer (ì™„ì„±ë„ ë†’ìŒ)
- **LSTM-SSTD**: https://github.com/jmichankow/deep_learning_probability
- **T-KAN**: arXiv:2601.02310 (ì½”ë“œ ì œê³µ ì˜ˆìƒ)
- **THGNN**: arXiv:2601.04602 (ìƒì„¸ ì„¤ëª…)

***

**ìµœì¢… í‰ê°€**: ì´ 5ê°œ ë…¼ë¬¸ì€ **2025-2026 ê¸ˆìœµ AIì˜ ìµœì „ì„ **ì„ ëŒ€í‘œí•©ë‹ˆë‹¤. íŠ¹íˆ í•œêµ­ ì‹œì¥(KOSPI, ì—…ë¹„íŠ¸)ì— ì§ì ‘ ì ìš© ê°€ëŠ¥í•œ ê¸°ìˆ ë“¤ë¡œ, 3ê°œì›” ë‚´ ì›í˜•(prototype) ê°œë°œ ì¶©ë¶„íˆ ê°€ëŠ¥í•©ë‹ˆë‹¤. ğŸš€
<span style="display:none">[^16][^17][^18][^19][^20][^21][^22][^23][^24][^25][^26][^27][^28][^29][^30][^31][^32][^33][^34][^35][^36][^37][^38][^39][^40][^41][^42][^43][^44][^45][^46][^47][^48][^49][^50][^51][^52][^53][^54][^55]</span>

<div align="center">â‚</div>

[^1]: https://arxiv.org/abs/2502.06151

[^2]: https://arxiv.org/html/2502.06151v1

[^3]: https://pmc.ncbi.nlm.nih.gov/articles/PMC12453695/

[^4]: https://arxiv.org/html/2508.18921

[^5]: https://www.academia.edu/122715084/An_Overview_of_Machine_Learning_Deep_Learning_and_Reinforcement_Learning_Based_Techniques_in_Quantitative_Finance_Recent_Progress_and_Challenges

[^6]: https://peerj.com/articles/cs-3001/

[^7]: https://arxiv.org/html/2508.18921v1

[^8]: https://www.themoonlight.io/fr/review/forecasting-probability-distributions-of-financial-returns-with-deep-neural-networks

[^9]: https://www.themoonlight.io/en/review/forecasting-probability-distributions-of-financial-returns-with-deep-neural-networks

[^10]: https://arxiv.org/html/2601.02310v1

[^11]: https://arxiv.org/pdf/2601.02310.pdf

[^12]: https://arxiv.org/abs/2601.02310

[^13]: https://pmc.ncbi.nlm.nih.gov/articles/PMC12563745/

[^14]: https://arxiv.org/html/2601.04602v1

[^15]: https://arxiv.org/abs/2601.04602

[^16]: https://arxiv.org/abs/2503.21422

[^17]: https://www.reddit.com/r/quant/comments/18csq03/quant_research_of_the_week_5th_edition/

[^18]: https://www.geeksforgeeks.org/deep-learning/transformer-for-time-series-forecasting/

[^19]: https://www.reddit.com/r/quant/comments/186r4to/quant_research_of_the_week_4th_edition/

[^20]: https://openreview.net/forum?id=kHEVCfES4Q\&noteId=mrNbq9EkQa

[^21]: https://github.com/dkanungo/Probabilistic-ML-for-finance-and-investing

[^22]: https://github.com/Leefinance/Quantitative-finance-papers-using-deep-learning

[^23]: https://arxiv.org/pdf/2508.18921.pdf

[^24]: https://bitwiseinvestments.com/crypto-market-insights/the-year-ahead-10-crypto-predictions-for-2026

[^25]: https://coinpedia.org/research-report/exclusive-report-crypto-market-predictions-2026/

[^26]: https://www.shadecoder.com/topics/transformer-for-time-series-a-comprehensive-guide-for-2025

[^27]: https://www.svb.com/industry-insights/fintech/2026-crypto-outlook/

[^28]: https://github.com/khegazy/Powerformer

[^29]: https://arxiv.org/abs/2508.18921

[^30]: https://www.nasdaq.com/articles/4-cryptocurrency-predictions-2026

[^31]: https://icml.cc/virtual/2025/poster/44262

[^32]: https://arxiv.org/html/2508.18921v2

[^33]: https://www.sciencedirect.com/science/article/abs/pii/S156849462500540X

[^34]: https://arxiv.org/list/q-fin/current

[^35]: http://www.hit.bme.hu/~fogarasi/NNOptionPricing.pdf

[^36]: https://arxiv.org/list/q-fin/2026-01?skip=100

[^37]: https://www.arxiv.org/pdf/2506.06657.pdf

[^38]: http://scholar-press.com/uploads/papers/Mk8vcZL9g8hM8OKN7nJffCPSNaboRGtFIa1oaOi2.pdf

[^39]: https://arxiv.org/list/q-fin.CP/current

[^40]: https://skemman.is/bitstream/1946/50779/1/qStorm_Thesis-1.pdf

[^41]: https://www.frontiersin.org/journals/physics/articles/10.3389/fphy.2025.1627551/full

[^42]: https://arxiv.org/list/q-fin.CP/2026-01

[^43]: https://onlinelibrary.wiley.com/doi/abs/10.1002/1099-131x(200007)19:4<299::aid-for775>3.0.co;2-v

[^44]: https://www.sciencedirect.com/science/article/abs/pii/S0950705125008263

[^45]: https://arxiv.org/abs/2510.19950

[^46]: https://users.ox.ac.uk/~mast0315/QuRegNeuralNet.pdf

[^47]: https://ieeexplore.ieee.org/document/10936350/

[^48]: https://coincodex.com/stock/EMAT/price-prediction/

[^49]: https://www.morningstar.com/stocks/xnas/emat/quote

[^50]: https://www.academia.edu/145355935/A_Novel_Hybrid_Temporal_Fusion_Transformer_Graph_Neural_Network_Model_for_Stock_Market_Prediction

[^51]: https://finance.yahoo.com/news/data-center-transformer-global-market-111800173.html

[^52]: https://www.sciencedirect.com/science/article/abs/pii/S0957417425029082

[^53]: https://ui.adsabs.harvard.edu/abs/2025Entrp..27.1029C/abstract

[^54]: https://ashpress.org/index.php/jcts/article/download/230/183/379

[^55]: https://www.emergentmind.com/topics/high-frequency-trading-hft-environments

