<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# 2026-01-29 AI × 금융 테크 업데이트 리포트

AI/퀀트 전공 대학원생용 (15–30분 분량)

***

## 1. 한눈에 보는 Big Picture

1. 트렌드 축
    - **시계열 파운데이션 모델(Time-series FMs)**: TimesFM, UniTS, MOMENT 등 거대 Transformer 기반 시계열 모델이 논문·산업 양쪽에서 표준으로 올라오는 중.[^1]
    - **확률분포 예측(Probabilistic forecasting)**: Transformer + 확률적 생성 모델(특히 diffusion, quantile/interval 기반) 조합이 메인 스트림으로 부상.[^2][^3][^4]
    - **금융 특화 LLM/파운데이션 모델**: 글로벌은 Financial AI 서베이, 국내는 FinKRX, ₩ON 등 한국 금융 특화 LLM이 등장해 재무제표/공시·질의응답 벤치마크를 장악하는 방향.[^5][^6][^7][^8][^9]
    - **크립토 마이크로스트럭처 + ML**: 고빈도 LOB 데이터 기반으로 “모델 복잡도 < 인풋 퀄리티”라는 메시지가 강해짐.[^10][^11][^12]
2. 실무/실습 축
    - **오픈소스 시계열 Transformer·확률 모델 코드**: TMDM (Transformer-Modulated Diffusion for probabilistic TS), PatchTST·iTransformer·Pyraformer 계열, Ant International Falcon TST 오픈소스.[^13][^14][^4][^15]
    - **국내/한글 금융 자원**: KRX-Bench, FinKRX, ₩ON Reasoning 모델 및 데이터셋 공개.[^6][^7][^8][^9]
    - **실제 데이터 소스**: Upbit/Bybit LOB, Amberdata/Tardis·코인데스크·Coindesk/Amberdata 등 크립토 레벨2 데이터.[^16][^17][^18][^19][^20][^21]

이 리포트는 (1) 최신 연구·산업 트렌드 요약 → (2) 트랜스포머 기반 확률분포 예측 핵심 논문·팁 → (3) 금융/크립토 실습용 오픈소스·데이터 → (4) 한국/글로벌 AI 금융 리소스 로 정리한다.

***

## 2. 트랜스포머 기반 확률분포 예측: 핵심 논문·아이디어

### 2.1 대표 논문・모델 큐레이션

| 축 | 논문/모델 | 포인트 | 실습 난이도 |
| :-- | :-- | :-- | :-- |
| Diffusion×TS | **TMDM – Transformer‑Modulated Diffusion Models for Probabilistic MTS Forecasting (ICLR 2024)**[^2][^4] | Transformer로 컨디션을 뽑고 diffusion으로 **전체 분포**를 복원하는 Bayesian 생성 프레임워크. 기존 “포인트 예측 + Gaussian noise”를 넘어서, 예측구간(PICP, QICE) 수준에서 SOTA 성능. | 중상 (PyTorch diffusion·TS 둘 다 이해 필요) |
| Quantile×Transformer | **QuantileFormer – Probabilistic TS Forecasting with Pattern‑Mixture Decomposition (IJCAI 2025)**[^3] | 시계열을 **quantile drift + divergence pattern + Gaussian residual**로 분해하고, Transformer가 quantile 패턴을 직접 학습. 다양한 quantile을 동시에 예측. | 중 (기존 Transformer 코드에 quantile loss 추가하는 수준으로 확장 가능) |
| Long-horizon TS | **PatchTST – A Time Series is Worth 64 Words (ICLR 2023)**[^13][^4] | 시계열 패치를 토큰처럼 만들어 긴 구간을 효율적으로 예측. 여러 probabilistic head(quantile, mixture 등)를 얹기 좋음. | 중 (코드 공개, 커스터마이즈 용이) |
| TS FMs 서베이 | **Survey of Transformer Networks for Time Series Forecasting (2024)**[^15] | TS-Transformer 계열(Informer, Pyraformer, PatchTST, iTransformer 등) 구조/장단·응용 도메인을 한 번에 정리. | 하 (설계 아이디어 참고용) |

이 중 “확률 분포”에 직접적인 건 **TMDM, QuantileFormer**이고, **PatchTST + probabilistic head 자체 구현**은 실습용으로 매우 좋다.

***

### 2.2 실전 설계 팁: 확률분포 예측 파이프라인

연구/실무 둘 다에서 통하는 설계 패턴을 정리하면:

1. **포인트 예측 베이스라인 선정**
    - 금융 TS에서는 PatchTST / iTransformer / TFT(TimeFusion Transformer)류가 강력 후보.[^15][^13]
    - 베이스라인은 먼저 **MSE/MAE/Directional Accuracy** 기준으로 충분히 튜닝.
2. **분포 예측 전략 선택**
    - (A) **Quantile Regression Head (QR)**: 입력은 PatchTST encoder output, 출력은 `(Horizon, Q)` shape. Pinball loss로 학습.
    - (B) **Mixture Density Network (MDN)**: mean/variance + mixture weight를 출력 (Gaussian mixture 등).
    - (C) **Diffusion / Generative**: TMDM처럼 Transformer representation을 condition으로 해서 diffusion decoder가 미래 시계열 샘플을 생성.[^4][^2]
3. **평가 지표 설계**
    - Point: RMSE, MAE, MAPE, Sharpe 개선 여부.[^22][^23]
    - Distribution:
        - **PICP (Prediction Interval Coverage Probability)**, **QICE (Quantile Interval Coverage Error)** – TMDM에서 제안.[^4]
        - CRPS, pinball loss 등.
4. **금융 특화 고려사항**
    - **Fat-tail, Regime shift**: 단순 Gaussian 가정보다는 quantile/mixure/diffusion 형태가 유리.
    - **조건부 분포**: macro・뉴스・마이크로스트럭처(feature) 등을 conditioning으로 넣을 것.[^24][^12][^10]

***

### 2.3 실습용 구현 전략 (연구 아이디어 포함)

연구/실습 아이디어를 구체적으로 제안하면:

1. **PatchTST + Quantile Head on Upbit/주가**
    - 데이터: Upbit OHLCV (분/틱), KRX 주가/ETF 등.
    - 구조:
        - 인코더: PatchTST(채널 독립 or 공동)로 과거 return/volume 등 인코딩.
        - 디코더: 각 horizon별로 0.05/0.5/0.95 quantile 출력.
    - Loss: pinball loss. 구간 커버리지 (e.g. 90% interval) vs 실제 관측을 PICP로 검증.[^3][^4]
2. **Micostructure Feature + 간단 Probabilistic Head**
    - LOB 기반 연구처럼 **order book imbalance, depth, spread, VPIN 유사 지표**를 feature로 만들고:[^11][^12][^10]
        - 출력: 다음 1s/5s midprice 변화의 분포 (e.g. Gaussian mixture).
        - 포인트는 “모델 복잡도보다 feature engineering”이라는 최근 크립토 LOB 논지.[^11]
3. **TMDM Adaptation to Finance**
    - 오리지널 TMDM 코드는 일반 TS 데이터셋에 대해 diffusion + Transformer를 결합해 분포 예측을 수행.[^2][^4]
    - 금융 응용 아이디어:
        - 다변량: 여러 종목 + factor + macro를 한 번에 입력.
        - 다운스트림: **VaR/ES, 옵션 가격(기저자산 경로 샘플링)**에 TMDM 샘플 사용.

***

## 3. 금융 AI 최신 동향 \& 필수 논문

### 3.1 금융 AI 서베이・리뷰

1. **A Survey of Financial AI: Architectures, Advances and Open Challenges (2024)**[^5]
    - 내용: 파운데이션 모델, 그래프기반, 강화학습, 알고트레이딩, 리스크 관리 등 Financial AI 전체 맵.
    - Takeaway:
        - 금융 전용 파운데이션 모델 (TS·텍스트 둘 다)이 “플랫폼 레이어”가 되고,
        - 상단에 **포트폴리오 최적화, 옵션/파생상품, 마켓 메이킹** 등의 down-stream task가 얹히는 구조.
2. **The Role of AI in Financial Forecasting: ChatGPT’s Potential and Challenges (2024)**[^25][^26][^27]
    - LLM/ChatGPT를 forecasting pipeline에 쓰는 패턴 (feature 엔지니어링, 시나리오 생성, 리스크 스토리텔링 등).
3. **Large Language Models for Financial Aid in Financial Time-series Forecasting (2024)**[^28]
    - GPT-계열을 **few-shot/zero-shot TS forecasting**에 사용해 전통 TS 모델을 outperform하는 케이스 제시.

연구자로서:

- “모든 것에 DL” 시대는 끝,
- **파운데이션 모델 + 도메인 특화 구조/데이터 + 불확실성 추정**이 핵심 모티브라는 점을 잡고 가면 좋다.

***

### 3.2 시계열 파운데이션 모델(FMs)

1. **TimesFM, UniTS, MOMENT 등**[^1]
    - TimesFM: patch 기반 decoder-only transformer, 여러 도메인의 혼합 데이터로 학습된 long-horizon forecaster.
    - UniTS/MOMENT: task tokenization, masked TS modeling 등으로 다도메인·멀티태스크 학습.
2. **Ant International Falcon TST (Time-Series Transformer) 오픈소스**[^14]
    - 최대 25억 파라미터, Mixture-of-Experts 아키텍처, multiple patch tokenizers.
    - 날씨·이벤트·금융·교통 등 다양한 TS에 SOTA zero-shot 성능.[^14]

연구/실습 포인트:

- TimesFM/UniTS 계열은 직접 Pretrained weight를 가져다 쓰기보다는,
**“패치+decoder-only+multi-domain training” 구조**를 벤치마크로 삼아,
국내 금융/Upbit 데이터로 미니버전 FM을 만드는 방향을 생각해볼 만하다.

***

### 3.3 금융+트랜스포머 응용: 주가·리스크·마이크로스트럭처

1. **Stock Forecasting with Transformers**
    - **Machine learning for stock return prediction: Transformers or simple neural networks (Finance Research Letters)** – Transformer가 1M/3M/12M 리턴 예측에서 전통 NN 대비 유의미한 우위.[^23]
    - **Financial Time Series Forecasting using CNN and Transformer (2023)** – CNN이 로컬 패턴, Transformer가 글로벌 컨텍스트를 잡는 하이브리드 구조 제안.[^29]
2. **멀티모달 + Sentiment**
    - **LSTM + Transformer-based Sentiment for Stock Price Prediction (2025)**[^24]
        - LSTM으로 price TS, FinBERT로 뉴스 감성, 통합 예측.
        - 실증적으로 event-driven 구간에서 directional accuracy 향상.
3. **크립토 마이크로스트럭처**
    - **Microstructure and Market Dynamics in Crypto Markets (Easley et al.)** – VPIN, Roll measure 등 마이크로스트럭처 지표가 BTC·ETH 가격에 유의미한 explanatory power.[^12][^10]
    - **Exploring Microstructural Dynamics in Crypto Limit Order Books (2025)** – Bybit L2 LOB 100ms 스냅샷을 활용해, 모델 복잡도보다 **입력 특성·전처리의 중요성**을 강조.[^11]
    - **Kimchi Premium Nonlinear Dynamics (2024)** – Upbit 마켓 프리미엄과 환율, 시총 등 요인이 비선형적으로 얽혀 있음을 분석.[^30]

연구 아이디어:

- Upbit LOB + VPIN/imbalance 등의 마이크로스트럭처 피처를 기반으로,
**트랜스포머 기반 확률적 midprice 분포 예측**을 하면 국제적으로도 경쟁력 있는 주제가 될 수 있다.

***

## 4. 한국 금융 AI / 한글 리소스

### 4.1 한국어 금융 LLM \& 벤치마크

1. **FinKRX (한국거래소 × OneLine AI 금융 LLM)**[^6]
    - KRX 공시·재무 데이터 약 20만 세트 중 8.6만 고품질 데이터를 사용, SFT + DPO로 금융 QA/분석 특화.[^6]
    - 2024년 “KRX Financial Language Model Performance Evaluation Competition”에서 사용된 데이터와 평가 방법이 기반.
2. **KRX-Bench (Financial Benchmark Creation via LLMs)**[^7]
    - 미국/일본/한국 3국 상장사 연간보고서로부터 1,002개 QA를 자동생성.
    - 향후 자동 업데이트로 금융 LLM 벤치마크를 동적으로 유지하는 방향 제시.
3. **₩ON – Korean Financial LLM (KRX-Data / Hugging Face)**[^8][^9]
    - 한국 금융 도메인 질의응답·추론에 특화된 공개 LLM.
    - 2단계 구조화 추론 (self-correcting reasoning + summary), SFT + DPO 파이프라인으로 학습.[^8]
    - KRX-Bench 포함 한국·일본·미국 회사 관련 QA에서 상위 성능.[^9]

실습 팁:

- HuggingFace `KRX-Data/WON-Reasoning` 모델을 로컬/Colab에서 띄워,
    - **재무제표 요약**,
    - **한국 종목 애널리스트 리포트 요약**,
    - **Quant 모델 설명 생성** 등에 활용 가능.

***

### 4.2 한국 금융 데이터·연구

1. **국내 상장사 패널 데이터 + 인과 분석**
    - **Discovering causal relationships among financial variables (Korea 제조업 227개사 패널)**.[^31]
        - 재무변수 간 인과관계를 구조적으로 추론하는 사례.
        - 향후 **TS foundation model + causal regularization** 같은 연구로 확장 가능.
2. **환율·김치 프리미엄 분석**
    - **Nonlinear Dynamics of Kimchi Premium** – Upbit 마켓 지수, 환율, turnover, market cap 등과의 비선형 관계 분석.[^30]
    - 이 논문 데이터/방법을 기반으로 **Transformer 기반 비선형 동학 + 확률분포 예측**으로 확장 가능.

***

## 5. 실습 리소스: 코드, 데이터, 플랫폼

### 5.1 크립토 시계열 / LOB 데이터

1. **Upbit**
    - 공식 HTTP/Websocket API
        - Python·여러 언어용 Swagger 기반 클라이언트: `upbit-exchange/client`.[^32]
        - 간단 Python용 REST/WS 래퍼: `miroblog/upbit_api_collection` – OHLCV 수집, 주문 API, L2 일부 기능 지원.[^33]
    - 외부 데이터 제공자:
        - **Tardis.dev – Upbit historical WebSocket data**:
            - 매달 1일 CSV 무료 샘플, full history는 유료. LOB 스냅샷/거래 데이터 포맷 문서화.[^20]
        - **Amberdata – Upbit Spot data now available (2025)**:
            - REST/Websocket/S3 bulk로 Upbit 시세·거래·orderbook snapshot 제공.[^18][^19]
2. **Bybit**
    - REST L2 orderbook snapshot API: `/v5/market/orderbook`, 1000 depth까지.[^16]
    - Websocket L2 stream: 다양한 깊이(depth) 스트림과 업데이트 레이트를 문서화.[^17]
    - 실습:
        - 단기(100ms–1s) midprice 움직임 예측, market/limit order 슬리피지 분포 예측 등.[^17][^16][^11]
3. **글로벌 Order book provider**
    - Coindesk Data / Amberdata / CryptoCompare 등에서 orderbook snapshot, L2 업데이트 스트림 서비스.[^19][^21]
    - 연구/학위 논문 수준이면 **직접 Websocket 수집 + 자체 정제**가 더 유연함(정규화 이슈).[^34]

***

### 5.2 금융 텍스트/문서 데이터

1. **KRX·DART**
    - KRX-Bench/₩ON 논문에서 DART 공시를 긁어 QA를 자동생성하는 pipeline이 자세히 설명된다.[^7][^9]
    - 참고용 실습:
        - Python + `dart_fss` (비공식 라이브러리) 또는 직접 HTTP로 DART XML/JSON 수집.
        - 공시 → LLM (₩ON, GPT-4 등)으로 질의응답/요약 태스크 구축.
2. **국제 금융 텍스트**
    - 10-K/10-Q/8-K: EDGAR를 통해 크롤링 후,
        - Financial QA, Risk factor summarization, LLM 평가용 벤치마크로 활용.

***

### 5.3 시계열 / 확률 모델 오픈소스

1. **TMDM 공식 코드 (ICLR 2024)**[^2][^4]
    - PyTorch 기반 diffusion + Transformer 프레임워크.
    - 6개 TS 데이터셋에 대한 분포 예측 실험 코드 포함.
    - 금융 실습:
        - 데이터 로더만 교체해 **ETF·선물·크립토** 데이터에 적용.
        - 샘플링된 미래 경로로 VaR/ES, 옵션 가격 Monte Carlo 시뮬레이션.
2. **PatchTST / Pyraformer / iTransformer 코드**[^13][^3][^15]
    - GitHub 상 다수 구현 (논문 레퍼런스에서 링크).
    - 포인트 예측 구조를 유지한 채 **output layer만 quantile/mixture/diffusion condition**으로 확장하는 것이 연구 효율 좋음.
3. **Ant Falcon TST (Time-Series Transformer)**[^14]
    - Ant International이 오픈소스 공개를 발표.[^14]
    - MoE + patch tokenizers로 구성된 대형 모델.
    - 실제 코드/모델은 GitHub/HF 링크를 통해 제공 (기사 내 링크 참고).

***

### 5.4 플랫폼: 실험/배포 환경

- **로컬/서버**
    - Ubuntu + PyTorch + CUDA. LOB·orderflow 다루면 디스크 I/O와 메모리 고려.
- **Colab / Kaggle / Paperspace**
    - 데이터 용량 제약이 있으므로, LOB는 샘플 기간(예: 1개월, 특정 심볼) 단위로 실험.
- **클라우드**
    - GCP/AWS + BigQuery/S3, 특히 Amberdata/Tardis에서 S3 연동 시 편리.[^18][^19][^20]

***

## 6. 연구자로서 “꼭 알아야 할” 기술/연구 업데이트 (체크리스트)

### 6.1 이론·모델 레벨

1. **시계열 Transformer 설계 패턴 (Encoder-only vs Decoder-only vs Encoder–Decoder)**[^15][^13]
2. **Patch/Tokenization 전략** – PatchTST/TimesFM/UniTS/MOMENT.[^13][^1][^15]
3. **확률분포 예측 프레임워크**
    - Quantile regression (QuantileFormer, MQ-Transformer 계열).[^3]
    - Diffusion for TS (TMDM 등).[^4][^2]
    - GAN/cGAN 기반 금융 예측 (Generative AI + 금융 예측 연구).[^26][^35]
4. **금융 AI 파운데이션 모델**
    - Global: Financial AI survey, TimesFM/UniTS 등.[^5][^1][^15]
    - Korea: FinKRX, ₩ON, KRX-Bench.[^9][^7][^8][^6]

### 6.2 데이터·도메인 레벨

1. **마이크로스트럭처** – VPIN, Roll measure, depth imbalance, spread, cancel/modify dynamics.[^36][^10][^12][^11]
2. **비선형·정책/환율 영향** – 김치 프리미엄·환율·시총 간 관계.[^31][^30]
3. **리스크/규제 측면** – AI 기반 금융 예측에 대한 explainability, robustness, fairness 논의.[^37][^38][^5]

***

## 7. 구체적 실습 플랜 제안 (3–6개월 연구 로드맵)

### 7.1 목표 예시 1: “Upbit LOB 기반 트랜스포머 확률분포 예측”

1. **데이터 수집**
    - Bybit L2 snapshot/stream로 프로토타입 (REST/Websocket).[^16][^17]
    - 필요시 Amberdata/Tardis로 Upbit 수준으로 확장.[^20][^18]
2. **Feature Engineering**
    - 첫 10~20 depth 수준의 **cumulative depth imbalance, spread, top-of-book volatility**, trade sign 등.[^21][^12][^11]
    - 시간 축 patching: 100ms–1s–5s 등.
3. **모델링**
    - Baseline: Conv1D+LSTM/DeepLOB류 (단기 midprice up/down 분류).[^11]
    - Advanced:
        - PatchTST encoder + quantile head (다음 1s/5s return 분포 예측).
        - 조건부 diffusion (TMDM 구조 차용)으로 전체 future path 샘플링.[^4]
4. **평가**
    - Hit rate, ROC-AUC (direction).
    - PICP/QICE, expected shortfall, slippage distribution.

***

### 7.2 목표 예시 2: “KRX 공시 기반 한글 금융 LLM + 시계열 연결”

1. **텍스트 레이어**
    - DART 공시 + 가격 데이터 연결.
    - ₩ON/FinKRX/기타 LLM으로 **이벤트 임베딩** 생성 (예: earnings surprise, risk narrative score).[^8][^9][^6]
2. **시계열 레이어**
    - Transformer 기반 시계열 모델에 이벤트 임베딩을 condition으로 주입.
    - 이벤트별 return distribution shift를 학습 (확률적 예측).
3. **연구 질문**
    - “LLM 기반 이벤트 피처가, 과거 이벤트 dummy보다 분포 예측 성능(PICP, CRPS)을 얼마나 개선하는가?”

***

## 8. 마무리: 오늘 기준으로 무엇을 공부/실습하면 좋은가

1. **논문 1차 필독 (이론/모델)**
    - TMDM (ICLR 2024)[^2][^4]
    - QuantileFormer (IJCAI 2025)[^3]
    - PatchTST (ICLR 2023)[^13]
    - Survey of TS Transformers[^15]
    - Survey of Financial AI[^5]
2. **한글/국내 자원**
    - ₩ON 논문 + HuggingFace 모델 카드[^9][^8]
    - KRX-Bench 논문[^7]
    - Kimchi Premium 비선형 동학 논문[^30]
    - 한국 제조업 패널 인과관계 논문[^31]
3. **실습용 코드/데이터**
    - Upbit API 클라이언트 + Tardis/Amberdata.[^32][^18][^20]
    - Bybit L2 orderbook REST/WS.[^17][^16]
    - TMDM 및 PatchTST GitHub (논문 참고).[^13][^4]
4. **연구 방향**
    - “**트랜스포머 기반 시계열 파운데이션 모델 × 확률분포 예측 × 한국/크립토 금융 도메인**”을 교차시키는 주제를 잡으면,
        - 신진 연구 트렌드에도 맞고,
        - 실전 데이터(Upbit/환율/국내 주식)와도 잘 맞는다.

원하면 다음 단계로,

- 특정 논문(TMDM/QuantileFormer/₩ON) 하나를 골라 수식·코드 레벨 요약 + 직접 돌릴 수 있는 미니멀 PyTorch 스켈레톤까지 정리해주는 “실습 가이드”를 따로 만들어 줄 수 있다.
<span style="display:none">[^39][^40][^41][^42][^43][^44][^45][^46][^47][^48][^49][^50][^51][^52][^53][^54][^55][^56][^57][^58][^59][^60][^61][^62][^63][^64][^65][^66]</span>

<div align="center">⁂</div>

[^1]: https://arxiv.org/html/2508.16641v1

[^2]: https://openreview.net/forum?id=qae04YACHs

[^3]: https://www.ijcai.org/proceedings/2025/0684.pdf

[^4]: https://proceedings.iclr.cc/paper_files/paper/2024/file/516a9317af9d89e9f2251bd7fde49b8f-Paper-Conference.pdf

[^5]: http://arxiv.org/pdf/2411.12747.pdf

[^6]: https://www.venturesquare.net/en/976316/

[^7]: https://aclanthology.org/2024.finnlp-1.2.pdf

[^8]: https://huggingface.co/KRX-Data/WON-Reasoning

[^9]: https://arxiv.org/html/2503.17963v1

[^10]: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4814346

[^11]: https://arxiv.org/html/2506.05764v1

[^12]: https://stoye.economics.cornell.edu/docs/Easley_ssrn-4814346.pdf

[^13]: http://arxiv.org/pdf/2211.14730v2.pdf

[^14]: https://financialit.net/news/artificial-intelligence/ant-international-open-sources-time-series-transformer-ai-model-enable

[^15]: https://www.sciencedirect.com/science/article/pii/S1574013725001595

[^16]: https://bybit-exchange.github.io/docs/v5/market/orderbook

[^17]: https://bybit-exchange.github.io/docs/v5/websocket/public/orderbook

[^18]: https://docs.amberdata.io/changelog/upbit-spot-data-now-available

[^19]: https://www.amberdata.io/order-book

[^20]: https://docs.tardis.dev/historical-data-details/upbit

[^21]: https://data.coindesk.com/order-book

[^22]: https://electuresai.com/transformer-ai-financial-market-prediction-2025/

[^23]: https://phys.org/news/2025-11-ai-outperform-neural-networks-stock.html

[^24]: https://www.anserpress.org/journal/jea/4/3/109

[^25]: https://arxiv.org/pdf/2411.13562.pdf

[^26]: https://arxiv.org/pdf/2404.03523.pdf

[^27]: https://arxiv.org/pdf/2304.07619.pdf

[^28]: https://arxiv.org/html/2410.19025v1

[^29]: https://arxiv.org/pdf/2304.04912.pdf

[^30]: https://www.sciencedirect.com/science/article/pii/S0264999324000828

[^31]: https://www.aimspress.com/article/doi/10.3934/DSFE.2025001?viewType=HTML

[^32]: https://github.com/upbit-exchange/client

[^33]: https://github.com/miroblog/upbit_api_collection

[^34]: https://www.reddit.com/r/quant/comments/1j9uyuj/how_do_you_access_l2_order_book_data_for_crypto/

[^35]: https://ace.ewapublishing.org/media/7b34f29f569b42a4a860c95856bed70a.marked.pdf

[^36]: https://unitesi.unive.it/retrieve/eed2f223-f3d3-459e-b4a6-25f233437bde/893488-1286715.pdf

[^37]: https://www.semanticscholar.org/paper/e1d10fd2dcc1fe028a9ec79217a2b5ffa7ae4449

[^38]: https://ieeexplore.ieee.org/document/11313393/

[^39]: https://ojs.wiserpub.com/index.php/CM/article/view/7331

[^40]: https://ijaem.net/issue_dcp/TransFed%20AI%20A%20Scalable%20and%20Private%20Architecture%20for%20Next%20Gen%20Forecasting.pdf

[^41]: http://poster-openaccess.com/article_detail.php?paper_id=846\&conf=ICIC\&year=2025

[^42]: https://arxiv.org/abs/2511.11090

[^43]: https://onlinelibrary.wiley.com/doi/10.1002/tee.70024

[^44]: https://www.ijircst.org/view_abstract.php?title=RfGANNNet-2.0:-A-Hybrid-AI-Framework-Integrating-Random-Forests,-Spatio-Temporal-Graph-Convolution,-and-Physics-Guided-GANs-for-High-Resolution-Rainfall-Forecasting\&year=2025\&vol=13\&primary=QVJULTEzNzk=

[^45]: https://ieeexplore.ieee.org/document/11309510/

[^46]: https://finance.yahoo.com/news/united-states-transformer-industry-forecast-122900869.html

[^47]: https://www.gminsights.com/industry-analysis/transformer-optimized-ai-chip-market

[^48]: https://linkinghub.elsevier.com/retrieve/pii/S0920548922000344

[^49]: https://link.springer.com/10.1007/978-3-030-50578-3_28

[^50]: https://meetingorganizer.copernicus.org/EMS2021/EMS2021-423.html

[^51]: https://academic.oup.com/nar/article/53/W1/W547/8126256

[^52]: https://www.semanticscholar.org/paper/8425177002de026cf91fe0aecdd5d5bc9d6c91c8

[^53]: https://ieeexplore.ieee.org/document/11108146/

[^54]: https://journal.arteii.or.id/index.php/Merkurius/article/view/997

[^55]: https://link.springer.com/10.1007/978-3-030-50578-3_36

[^56]: https://www.epiprev.it/6191

[^57]: https://link.springer.com/10.1007/978-3-030-50578-3_44

[^58]: https://linkinghub.elsevier.com/retrieve/pii/S0927025614003322

[^59]: http://arxiv.org/pdf/2404.19614.pdf

[^60]: https://research.wu.ac.at/ws/files/26694260/neum-etal-RW2017.pdf

[^61]: https://linkinghub.elsevier.com/retrieve/pii/S2153353923001645

[^62]: http://arxiv.org/pdf/2402.09615.pdf

[^63]: https://arxiv.org/pdf/1606.05832.pdf

[^64]: https://arxiv.org/ftp/arxiv/papers/2104/2104.05040.pdf

[^65]: http://arxiv.org/pdf/2403.19340.pdf

[^66]: https://pub.dev/documentation/upbit/latest/

