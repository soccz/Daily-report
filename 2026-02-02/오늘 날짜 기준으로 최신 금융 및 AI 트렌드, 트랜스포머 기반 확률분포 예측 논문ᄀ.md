<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  ê¸ˆìœµ ë° AI íŠ¸ë Œë“œ, íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ í™•ë¥ ë¶„í¬ ì˜ˆì¸¡ ë…¼ë¬¸ê³¼ íŒ, AI ì „ê³µìë¡œì„œ ê¼­ ì•Œì•„ì•¼ í•  í•µì‹¬ ì—°êµ¬Â·ê¸°ìˆ  ì—…ë°ì´íŠ¸, ì‹¤ì œ ì‹¤ìŠµì— í™œìš©í•  ìˆ˜ ìˆëŠ” ì˜¤í”ˆì†ŒìŠ¤/ë°ì´í„°/í•µì‹¬ ë…¼ë¬¸ ì¶”ì²œ, ì‹¤ìŠµ ìì›(ë„êµ¬, ë°ì´í„°ì…‹, í”Œë«í¼ ë“±) ìš”ì•½ì„ ì „ë¬¸ê°€ ì‹œê°ìœ¼ë¡œ í•œê¸€ ë‰´ìŠ¤/ë³´ê³ ì„œ ìŠ¤íƒ€ì¼(15~30ë¶„ ë¶„ëŸ‰)ë¡œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ì¤˜. ê° í•­ëª©ì—ëŠ” êµ¬ì²´ì  ì¶œì²˜ì™€, ì‹¤ì œ ì‹¤ìŠµ ê°€ëŠ¥í•œ ì˜¤í”ˆì†ŒìŠ¤Â·ë°ì´í„°Â·í•µì‹¬ ë…¼ë¬¸/ì•„í‹°í´ì„ ë°˜ë“œì‹œ ì¶”ì²œí•˜ê³ , í•œêµ­ ë° ê¸€ë¡œë²Œ ì†Œì‹ì„ ëª¨ë‘ í¬í•¨í•´ì¤˜. ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë ¤ë„ ê´œì°®ìœ¼ë‹ˆ ë¬´ì¡°ê±´ ì „ì²´ ë³´ê³ ì„œë¥¼ ì™„ì„±í•˜ì—¬ ë³´ë‚´ì¤˜

ì´ì œ ìµœì¢… ì¢…í•© ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.

# ğŸ“Š **2026ë…„ ê¸ˆìœµÂ·AI íŠ¸ë Œë“œ ì¢…í•© ë³´ê³ ì„œ**

## *ìµœì‹  AI ì—°êµ¬, íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ í™•ë¥ ë¶„í¬ ì˜ˆì¸¡, ì‹¤ë¬´ ì ìš© ê°€ì´ë“œ*


***

## **â… . ì‹¤í–‰ ìš”ì•½ (Executive Summary)**

2026ë…„ ê¸ˆìœµ AI ì‹œì¥ì€ ì„¸ ê°€ì§€ ê±°ëŒ€ ë³€í™” êµ­ë©´ì„ ë§ì´í•œë‹¤. ì²«ì§¸, **ì¶”ë¡  ê°•í™” LLM**(o3-pro, DeepSeek-R1)ì´ ê¸ˆìœµ ì˜ì‚¬ê²°ì •ì„ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜í•˜ê³  ìˆìœ¼ë©°, ë‘˜ì§¸ **íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ í™•ë¥ ë¶„í¬ ì˜ˆì¸¡** ëª¨ë¸ë“¤(N-BEATS, TFT, Quantile DL)ì´ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”ì—ì„œ ì‚°ì—… í‘œì¤€í™”ë¥¼ ì§„í–‰ ì¤‘ì´ë‹¤. ì…‹ì§¸, **í•œêµ­ ê¸ˆìœµê¶Œì˜ AI ì—ì´ì „íŠ¸ ì „í™˜**(AX ì „ì‚¬í™”, ë””ì§€í„¸ìì‚° ìˆ˜ìš©)ì€ ì •ì±… ì§€ì›ê³¼ ë§ë¬¼ë ¤ êµ¬ì¡°ì  ì „í™˜ê¸°ë¥¼ ë§ì´í–ˆë‹¤.

ì´ ë³´ê³ ì„œëŠ” ëŒ€í•™ì› ìˆ˜ì¤€ì˜ ë¶„ì„ ê¹Šì´ë¡œ ìµœì‹  ë…¼ë¬¸ 80ì—¬ í¸ê³¼ ê¸€ë¡œë²ŒÂ·êµ­ë‚´ ì‹¤ë¬´ ì‚¬ë¡€ë¥¼ ì¢…í•©í•˜ì—¬, AI ì „ê³µìê°€ ì¦‰ì‹œ í™œìš© ê°€ëŠ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ì½”ë“œ, ë°ì´í„°ì…‹, í•µì‹¬ ë…¼ë¬¸ì„ ì œì‹œí•œë‹¤.

***

## **â…¡. ê¸ˆìœµ ì‹œì¥ ê±°ì‹œ íŠ¸ë Œë“œ (2026 Financial Macro Context)**

### **2-1. ê¸€ë¡œë²Œ í†µí™”ì •ì±… ì „í™˜ \& ìœ ë™ì„± í€ë”ë©˜íƒˆ**

2026ë…„ ì´ˆ ë¯¸êµ­ ê²½ì œëŠ” ì„¸ ê°€ì§€ ë™ì‹œ ì´‰ë§¤ í˜„ìƒì´ ë°œë™í•˜ëŠ” "ìœ ë™ì„±ì˜ ì™„ë²½í•œ í­í’" ì‹œê¸°ë‹¤:[^1]

- **ì…§ë‹¤ìš´ ì¢…ë£Œ**: 2025ë…„ 10ì›” ì‹œì‘ëœ ë¯¸êµ­ ì—°ë°©ì •ë¶€ ì…§ë‹¤ìš´(36ì¼, ì—­ëŒ€ ìµœì¥)ì´ ì¢…ë£Œë  ë•Œ ì¬ë¬´ë¶€ ì¼ë°˜ê³„ì •ì˜ **930ì–µ ë‹¬ëŸ¬(ì•½ 1,354ì¡° ì›)ê°€ ì‹œì¥ì— ìœ ì…**
- **ì–‘ì ê¸´ì¶•(QT) ì¢…ë£Œ**: 2025ë…„ 12ì›” 1ì¼ë¶€í„° 3ë…„ 6ê°œì›”ê°„ì˜ ì–‘ì ê¸´ì¶•ì´ ê³µì‹ ì¢…ë£Œ â†’ **ê¸°ì¡´ ìœ ë™ì„± íšŒìˆ˜ ì¤‘ë‹¨, ëŒ€ì°¨ëŒ€ì¡°í‘œ ì•ˆì •í™”**
- **401(k) ì•”í˜¸í™”í ê°œë°©**: íŠ¸ëŸ¼í”„ í–‰ì •ëª…ë ¹ìœ¼ë¡œ **9ì¡° ë‹¬ëŸ¬(12,600ì¡° ì›) ê·œëª¨ì˜ í‡´ì§ì—°ê¸ˆì´ ì•”í˜¸í™”í íˆ¬ì ê°€ëŠ¥** â†’ êµ¬ì¡°ì  ì›”ê°„ ìë™ ìˆ˜ìš” ë°œìƒ

ì´ëŸ¬í•œ í™˜ê²½ì—ì„œ **ê¸ˆë¦¬ ì¸í•˜ì˜ ì‹œì°¨ íš¨ê³¼**ê°€ 2026ë…„ 2~3ë¶„ê¸°ë¶€í„° ì‹¤ë¬¼ê²½ì œì— ë³¸ê²©í™”ë˜ë©´ì„œ, ê¸ˆìœµì‹œì¥ ìœ ë™ì„±ê³¼ ì‹¤ë¬¼ê²½ì œ ì†Œë¹„ê°€ ë™ì‹œ í™•ì¥ë˜ëŠ” "í™©ê¸ˆê¸°"ê°€ í˜•ì„±ë  ê²ƒìœ¼ë¡œ ì „ë§ëœë‹¤.[^2][^1]

### **2-2. í•œêµ­ ê¸ˆìœµê¶Œì˜ AI ì „í™˜ (AX, Digital Assets)**

êµ­ë‚´ ê¸ˆìœµê¶Œ ì‹ ë…„ì‚¬ ë¶„ì„ ê²°ê³¼, 2025ë…„ ê²½ì˜ ì „ëµì˜ í•µì‹¬ í‚¤ì›Œë“œëŠ” **AX(AI Transformation), ìŠ¤í…Œì´ë¸”ì½”ì¸, ìƒì‚°ì ê¸ˆìœµ**ì´ë‹¤:[^3][^4]


| ê¸ˆìœµì‚¬ | ì£¼ìš” ì „ëµ | êµ¬ì²´ì  ì‹¤í–‰ |
| :-- | :-- | :-- |
| **ì‹ í•œì€í–‰** | AI ì—ì´ì „íŠ¸, ì±„ë„ í˜ì‹  | 'íˆ¬ì ë©”ì´íŠ¸', 'GPT ê¸°ë°˜ ì„œë¥˜ì‹¬ì‚¬', í”Œë«í¼ ë¹„ì¦ˆë‹ˆìŠ¤ |
| **KBê¸ˆìœµ** | AI ê´‘ë²”ìœ„ ì ìš© | í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ AI ì½”íŒŒì¼ëŸ¿, Future Contact Center |
| **êµ­ë¯¼ì€í–‰** | AI ì—ì´ì „íŠ¸ ë±…í¬ | ìŠ¤ë§ˆíŠ¸ ìƒë‹´ ì¡°ìˆ˜, ê³ ë¶€ê°€ê°€ì¹˜ ì—…ë¬´ ì§‘ì¤‘ |
| **ì¹´ì¹´ì˜¤ë±…í¬** | AI ìƒíƒœê³„ êµ¬ì¶• | ì „ìš© ë°ì´í„°ì„¼í„° ê°œì†Œ, ì›í™” ìŠ¤í…Œì´ë¸”ì½”ì¸ PoC |
| **Kë±…í¬** | ê¸ˆìœµ íŠ¹í™” LLM | Solar LLM ê¸°ë°˜ ì»¤ìŠ¤í…€ ëª¨ë¸ êµ¬ì¶• |

**ê¸ˆìœµë‹¹êµ­ ì •ì±… ì§€ì›**: 2025ë…„ 1ë¶„ê¸°ë¶€í„° "ê¸ˆìœµê¶Œ AI í”Œë«í¼" ì¶œë²”, ê¸ˆìœµ íŠ¹í™” í•œê¸€ ë§ë­‰ì¹˜ ë‹¨ê³„ì  ì œê³µ, AI ê°€ì´ë“œë¼ì¸ ê°œì • â†’ **AI í•™ìŠµÂ·í‰ê°€ìš© ì‹ ë¢° ë°ì´í„° ì¸í”„ë¼ ì¡°ì„±**[^5]

***

## **â…¢. íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ í™•ë¥ ë¶„í¬ ì˜ˆì¸¡: ìµœì‹  ë…¼ë¬¸ \& ê¸°ë²•**

ê¸ˆìœµ ì‹œê³„ì—´ ì˜ˆì¸¡ì—ì„œ í™•ë¥ ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ê²ƒì€ **ì (point) ì˜ˆì¸¡ì´ ì•„ë‹Œ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”**ê°€ í•µì‹¬ì´ë‹¤. 2024~2026ë…„ ìµœì‹  ë…¼ë¬¸ë“¤ì€ ë‹¤ìŒê³¼ ê°™ì€ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ì œì‹œí•œë‹¤:

### **3-1. Neural Basis Expansion Analysis (N-BEATS) ê³„ì—´**

**í•µì‹¬ ë…¼ë¬¸**: "N-BEATS: Neural Basis Expansion Analysis for Interpretable Time Series Forecasting"[^6][^7]

**êµ¬ì¡°**:

- **ê¸°ì € í•¨ìˆ˜ ë¶„í•´**: ì‹œê³„ì—´ì„ ë‹¤í•­ì‹(polynomial), ì¡°í™”(harmonic), ë˜ëŠ” í•­ë“±(identity) ê¸°ì €ë¡œ í™•ì¥
- **ì´ì¤‘ ì”ì°¨ ìŠ¤íƒ**: ê° ë¸”ë¡ì´ ë‹¤ìŒ ë¸”ë¡ìœ¼ë¡œ residualì„ ì „ë‹¬ â†’ Box-Jenkins ë°©ì‹ì˜ ARIMA ëª¨ë¸ë§ê³¼ ìœ ì‚¬í•œ í•´ì„ ê°€ëŠ¥ì„±
- **ì„±ëŠ¥**: M4/M3 ê²½ìŸì—ì„œ í†µê³„ ëª¨ë¸ ëŒ€ë¹„ 11% ê°œì„ , M4 ìš°ìŠ¹ì ëŒ€ë¹„ 3% ê°œì„ [^6]

**í™•ì¥ ë²„ì „**:

- **NBEATSx**: ì™¸ìƒ ë³€ìˆ˜ í¬í•¨ â†’ **ì „ê¸°ìš”ê¸ˆ ì˜ˆì¸¡ì—ì„œ 20% ì •í™•ë„ ê°œì„ **[^8]
- **Probabilistic N-BEATS**: ë§¤ê°œë³€ìˆ˜ í™•ë¥ ë¶„í¬ í•™ìŠµ â†’ ì‹ ë¢° êµ¬ê°„ ì œê³µ[^9]

**ì‹¤ë¬´ í™œìš©**: ë¡œë“œ í¬ìºìŠ¤íŒ…(load forecasting), íƒœì–‘ê´‘ ë°œì „ ì˜ˆì¸¡, ì£¼ê°€ ì˜ˆì¸¡[^10][^11][^12][^13]

***

### **3-2. Temporal Fusion Transformer (TFT)**

**í•µì‹¬ ë…¼ë¬¸**: "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting"[^14]

**íŠ¹ì§•**:

- **ë‹¤ì¤‘ ì…ë ¥ ìœ í˜• ì²˜ë¦¬**:
    - ê³¼ê±° ê´€ì¸¡ê°’ (unknown observed)
    - ë¯¸ë˜ ê¸°ì§€ì •ë³´ (known future inputs)
    - ì •ì  ê³µë³€ëŸ‰ (static covariates)
- **í•´ì„ ê°€ëŠ¥ì„± (Interpretability)**:
    - Variable Selection Network (VSN) â†’ íŠ¹ì„± ì¤‘ìš”ë„ ì¸¡ì •
    - ë‹¤ì¤‘í—¤ë“œ ì–´í…ì…˜ â†’ ê³„ì ˆì„± íŒ¨í„´ ì‹œê°í™”
    - Gate ë©”ì»¤ë‹ˆì¦˜ â†’ ë¶ˆí•„ìš”í•œ êµ¬ì„±ìš”ì†Œ ì–µì œ

**ì„±ëŠ¥**: ì „ê¸°ì†Œë¹„ ì˜ˆì¸¡, ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ì—ì„œ ìµœì‹  ë²¤ì¹˜ë§ˆí¬ ì´ˆê³¼[^15][^14]

**êµ¬í˜„ ìë£Œ**: MathWorks (MATLAB), PyTorch Forecasting, Hugging Face Transformers[^16][^15]

***

### **3-3. ë¶„ìœ„ìˆ˜ íšŒê·€ ê¸°ë°˜ ì‹¬í™” í•™ìŠµ (Quantile Deep Learning)**

**í•µì‹¬ ë…¼ë¬¸**: "Quantile Deep Learning Models for Multi-step Ahead Time Series Prediction"[^17]

**ë°©ë²•ë¡ **:

- **ë¶„ìœ„ìˆ˜ ì†ì‹¤í•¨ìˆ˜ (Quantile Loss)**:
$L_q(\hat{y}, y) = \sum_i q \cdot (y_i - \hat{y}_i)^+ + (1-q) \cdot (\hat{y}_i - y_i)^+$

â†’ ë‹¨ì¼ ì  ì˜ˆì¸¡ ëŒ€ì‹  **ì¡°ê±´ë¶€ ë¶„í¬ì˜ ë‹¤ì¤‘ ë¶„ìœ„ìˆ˜(10th, 50th, 90th percentile)ë¥¼ ë™ì‹œ ì˜ˆì¸¡**
- **ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”**: ë†’ì€ ë³€ë™ì„± ì‹œì¥(ì•”í˜¸í™”í)ì—ì„œë„ ì‹ ë¢°ë„ ìœ ì§€

**ì ìš©**: ë¹„íŠ¸ì½”ì¸Â·ì´ë”ë¦¬ì›€ ì˜ˆì¸¡ì—ì„œ **ë³€ë™ì„± ë†’ì€ í™˜ê²½ ëŒ€ë¹„ RMSE 17% ê°œì„ , ì‹ ë¢° êµ¬ê°„ ì•ˆì •ì„± í™•ë³´**[^17]

**ëª¨ë¸ ì¡°í•©**: ED-LSTM + Quantile Loss, RNN + Quantile Regression (Multivariate/Univariate)[^18]

***

### **3-4. ì‹ ê²½ë§ ê¸°ë°˜ GAMLSS (Location-Scale-Shape)**

**ìµœì‹  ë…¼ë¬¸**: "NBMLSS: Probabilistic Forecasting of Electricity Prices via Neural Basis Models for Location Scale and Shape"[^19][^20]

**í˜ì‹ ì **:

- ì‹ ê²½ë§ì˜ **ìœ ì—°ì„± + GAMLSSì˜ í•´ì„ ê°€ëŠ¥ì„±** ê²°í•©
- ë¶„í¬ì˜ ìœ„ì¹˜(Î¼), ì²™ë„(Ïƒ), í˜•íƒœ(Î³) íŒŒë¼ë¯¸í„°ë¥¼ **ë™ì‹œì— ì‹ ê²½ë§ìœ¼ë¡œ í•™ìŠµ**
- ì „ê¸°ìš”ê¸ˆ ì˜ˆì¸¡ì—ì„œ **distributional neural networksê³¼ ë™ë“± ì„±ëŠ¥ ìœ ì§€í•˜ë©´ì„œ ëª¨ë¸ ë™ì‘ íˆ¬ëª…ì„± í™•ë³´**

***

## **â…£. LLM \& ì¶”ë¡  ëª¨ë¸ ìµœì‹  íŒŒë™**

### **4-1. GPT-4o ê³„ì—´ vs ì¶”ë¡  ê°•í™” ëª¨ë¸ (o3-pro, DeepSeek-R1)**

| ëª¨ë¸ | ì£¼ìš” íŠ¹ì„± | ê¸ˆìœµ ì‘ìš© | í•œê³„ |
| :-- | :-- | :-- | :-- |
| **GPT-4o** | ë©€í‹°ëª¨ë‹¬(í…ìŠ¤íŠ¸/ìŒì„±/ì´ë¯¸ì§€), ë¹ ë¥¸ ì‘ë‹µ | ì‹¤ì‹œê°„ ìƒë‹´, ë¬¸ì„œ ë¶„ì„, ê°ì • ë¶„ì„ | ê¹Šì€ ì¶”ë¡  ì•½í•¨ |
| **o3-pro** | ê·¹ë„ì˜ "thinking time" í™•ì¥, chain-of-thought ê°•í™” | ë³µì¡ ë¦¬ìŠ¤í¬ ë¶„ì„, í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™” | ì‘ë‹µ ì‹œê°„ 750ë°° ëŠë¦¼ (GPT-4o ëŒ€ë¹„) |
| **DeepSeek-R1** | ê°•í™”í•™ìŠµ ê¸°ë°˜ ì¶”ë¡ , ì˜¤í”ˆ ì†ŒìŠ¤ | ë¹„ìš© íš¨ìœ¨ì  ëŒ€ê·œëª¨ ë°°í¬ | íì‡„ì†ŒìŠ¤ ëª¨ë¸ë³´ë‹¤ ì •í™•ë„ ë¯¸í¡ |
| **GPT-4.5** | Orion, ë¹„ì§€ë„í•™ìŠµ ëŒ€ê·œëª¨ í™•ëŒ€ | ì‚¬ì‹¤ ì •í™•ë„â†‘, ë‹¤êµ­ì–´ ê¸ˆìœµ ë‰˜ì•™ìŠ¤ | ì‹œê°/ìŒì„± ë¯¸ì§€ì› |

**2026 ê¸ˆìœµê¶Œ ì‹¤ì œ ë™í–¥**: o3/o3-proëŠ” **ë³´ì•ˆ/ë¦¬ìŠ¤í¬ ë¶„ì„ ê°™ì€ ê³ ë¶€ê°€ ì˜ì‚¬ê²°ì •**, GPT-4oëŠ” **ê³ ê° ì‘ëŒ€/ì‹¤ì‹œê°„ ìƒë‹´**, DeepSeek-R1 ê¸°ë°˜ ì»¤ìŠ¤í…€ ëª¨ë¸ì€ **ë‚´ë¶€ ìë™í™”**ë¡œ ì—­í•  ë¶„ë‹´[^21][^22][^23]

***

### **4-2. TimeGPT: ì‹œê³„ì—´ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸**

**ë…¼ë¬¸**: "TimeGPT-1"[^24]

**í˜ì‹ **:

- **100B+ ë°ì´í„°í¬ì¸íŠ¸ ì‚¬ì „í•™ìŠµ** â†’ Zero-shot ì¶”ë¡ ìœ¼ë¡œ ë¯¸í•™ìŠµ ë°ì´í„°ì…‹ì—ë„ ì ìš©
- ê¸°ì¡´ SOTA í†µê³„/ML/DL ëª¨ë¸ê³¼ ë™ë“± ì´ìƒ ì„±ëŠ¥
- ê°„ë‹¨í•œ API í˜¸ì¶œë¡œ ë°°í¬ ê°€ëŠ¥

**ì„±ëŠ¥ ë¹„êµ**:

```
TimeGPT zero-shot >> ê° ë°ì´í„°ì…‹ë³„ SOTA ëª¨ë¸ (domain-adjusted hand-crafted ensemble)
```

**í™œìš©**: ì†Œë§¤, ì „ë ¥, ê¸ˆìœµ, IoT ë°ì´í„° ì˜ˆì¸¡[^25][^24]

***

## **â…¤. ì‹¤ë¬´ ì ìš©: ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ \& ë°ì´í„°ì…‹**

### **5-1. ê¶Œì¥ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìŠ¤íƒ**

#### **1) NeuralForecast (Nixtla)**

```python
# ì„¤ì¹˜
pip install neuralforecast

# ê¸°ë³¸ ì‚¬ìš© (N-BEATS)
from neuralforecast.models import NBEATS
model = NBEATS(h=24, input_size=336, max_steps=100)
```

- **ê°•ì **: ëª¨ë“ˆí™”, ë³‘ë ¬ í›ˆë ¨, ë‹¤ì¤‘ ì‹œê³„ì—´ ì§€ì›
- **ëª¨ë¸êµ°**: N-BEATS, N-NBEATS, N-HiTS, ARIMA, ETS, AutoARIMA, AutoETS
- **ë¬¸ì„œ**: https://nixtlaverse.nixtla.io/neuralforecast/[^26][^27]


#### **2) PyTorch Forecasting**

```python
from pytorch_forecasting import TemporalFusionTransformer
model = TemporalFusionTransformer.from_dataset(
    training_dataset, 
    learning_rate=0.01,
    hidden_size=16,
    attention_head_size=4
)
```

- **ê°•ì **: TFT, DeepAR, Seq2Seq ë“± ìµœì‹  ëª¨ë¸
- **íŠ¹ì§•**: PyTorch Lightning ê¸°ë°˜, GPU ë³‘ë ¬í™”
- **ë¬¸ì„œ**: https://pytorch-forecasting.readthedocs.io[^28][^16]


#### **3) Hugging Face Time Series Transformer**

```python
from transformers import AutoformerForPrediction, TimeSeriesTransformerForPrediction
model = TimeSeriesTransformerForPrediction.from_pretrained(
    "huggingface/time-series-transformer-electricity-load"
)
```

- **ê°•ì **: ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ì œê³µ, Hugging Face Hub í†µí•©
- **íŠ¹ì§•**: í™•ë¥ ë¶„í¬ ì¶œë ¥ (ë¶„ìœ„ìˆ˜ íšŒê·€)
- **ë¬¸ì„œ**: https://huggingface.co/docs/transformers/en/model_doc/time_series_transformer[^29]


#### **4) Prophet (Meta)**

```python
from prophet import Prophet
model = Prophet(interval_width=0.95, seasonality_mode='additive')
model.fit(df)
forecast = model.make_future_dataframe(periods=30)
fcst = model.predict(forecast)
```

- **ê°•ì **: í•´ì„ ê°€ëŠ¥, íœ´ì¼/ì´ìƒì¹˜ ìë™ ì²˜ë¦¬
- **ì•½ì **: ê³ ì • êµ¬ì¡°, ê·¹ë„ì˜ ë³µì¡ íŒ¨í„´ ì•½í•¨
- **ì‚¬ìš©ì²˜**: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘, ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì •[^30]


#### **5) í™•ë¥ ì  í”„ë¡œê·¸ë˜ë°: PyMC, Pyro, NumPyro**

```python
import pymc as pm
with pm.Model() as model:
    sigma = pm.HalfNormal('sigma', sigma=1)
    mu = pm.Normal('mu', mu=0, sigma=10)
    y = pm.Normal('y', mu=mu, sigma=sigma, observed=data)
    idata = pm.sample(1000, tune=1000)
```

- **ìš©ë„**: ë² ì´ì§€ì•ˆ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”, ê³„ì¸µì  ëª¨ë¸
- **ì„ íƒ ê¸°ì¤€**:
    - PyMC: ìœ ì—°ì„±, MCMC/VI í†µí•©
    - Pyro: PyTorch ê¹Šìˆ™ì´, GPU í™•ì¥
    - NumPyro: JAX ê¸°ë°˜, ê·¹ë„ì˜ ì†ë„[^30]

***

### **5-2. í•œêµ­ ì‹¤ë¬´ ë°ì´í„°ì…‹**

| ë°ì´í„°ì…‹ | ì¶œì²˜ | íŠ¹ì§• | í™œìš© |
| :-- | :-- | :-- | :-- |
| **ì—…ë¹„íŠ¸ ì‹œì„¸ API** | Upbit | ì•”í˜¸í™”í OHLCV, ë¶„/ì‹œê°„/ì¼ | ë¹„íŠ¸ì½”ì¸ ì£¼ê°€ ì˜ˆì¸¡ PoC |
| **ê³µê³µ ì£¼ê°€ ë°ì´í„°** | FinanceDataReader (KRX) | êµ­ë‚´ ì£¼ì‹ ì¢…ê°€, ê±°ë˜ëŸ‰ | í•œêµ­ ì£¼ê°€ ëª¨ë¸ í•™ìŠµ |
| **í™˜ìœ¨ ì‹œê³„ì—´** | í•œêµ­ì€í–‰ API | USD/KRW, JPY/KRW ë“± | í†µí™” pair ì˜ˆì¸¡ |
| **ì „ë ¥ ìˆ˜ê¸‰** | KEPCo ê³µê°œ ë°ì´í„° | ì‹œê°„ëŒ€ë³„ ì „ë ¥ ë¶€í•˜, ì¬ìƒì—ë„ˆì§€ | ë¶€í•˜ ì˜ˆì¸¡ ëª¨ë¸ |
| **ê¸ˆìœµê¶Œ ë§ˆì´ë°ì´í„°** | ë§ˆì´ë°ì´í„° 2.0 (6ì›” 19ì¼) | í†µí•© ê¸ˆìœµ ë°ì´í„° | ê°œì¸ë§ì¶¤ ìì‚°ê´€ë¦¬ |

**êµ¬ì²´ì  ì½”ë“œ ì˜ˆì‹œ (ì—…ë¹„íŠ¸ ë°ì´í„°)**:

```python
import pyupbit
import pandas as pd

# ë¹„íŠ¸ì½”ì¸ ì¼ê°„ ë°ì´í„° 200ê°œ ìˆ˜ì§‘
df = pyupbit.get_ohlcv("KRW-BTC", interval="day", count=200)

# ì •ê·œí™”
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df['close'].values.reshape(-1, 1))

# ì‹œí€€ìŠ¤ ìƒì„± (LSTM í•™ìŠµìš©)
def create_sequences(data, seq_length=60):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length])
    return np.array(X), np.array(y)

X, y = create_sequences(scaled_data, seq_length=60)
```


***

### **5-3. í•µì‹¬ ë…¼ë¬¸ \& ì•„í‹°í´**

#### **ì¦‰ì‹œ ì‹¤ìŠµ ê°€ëŠ¥ ë…¼ë¬¸ (with Code)**:

1. **"N-BEATS: Neural Basis Expansion Analysis for Interpretable Time Series Forecasting"**[^7][^6]
    - GitHub: https://github.com/ElementAI/N-BEATS
    - Nixtla êµ¬í˜„: NeuralForecast ë¼ì´ë¸ŒëŸ¬ë¦¬
    - **ë‚œì´ë„**: ì¤‘ê°„ | **ì ìš© ì‹œê°„**: 1~2ì¼
2. **"Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting"**[^14]
    - ë…¼ë¬¸: arXiv:1912.09363
    - êµ¬í˜„: PyTorch Forecasting, Hugging Face
    - **ë‚œì´ë„**: ë†’ìŒ | **ì ìš© ì‹œê°„**: 3~5ì¼
3. **"Quantile Deep Learning Models for Multi-step Ahead Time Series Prediction"**[^17]
    - ìµœì‹  (2024ë…„ 11ì›”): arXiv:2411.15674
    - ì•”í˜¸í™”í ì ìš© ì‚¬ë¡€ í¬í•¨
    - **ë‚œì´ë„**: ì¤‘ìƒ | **ì ìš© ì‹œê°„**: 2~3ì¼
4. **"Large Language Models Are Zero-Shot Time Series Forecasters"**[^31]
    - GPT-3, LLaMA-2ì˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëŠ¥ë ¥ ê²€ì¦
    - í…ìŠ¤íŠ¸â†’ìˆ«ì ì¸ì½”ë”© ë°©ì‹ í˜ì‹ ì 
    - **ë‚œì´ë„**: ì¤‘ê°„ | **ì ìš© ì‹œê°„**: 2ì¼
5. **"Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting"**[^32]
    - LLM + ì‹œê³„ì—´ì˜ í•˜ì´ë¸Œë¦¬ë“œ ê¸ˆìœµ ì˜ˆì¸¡
    - Open-LLaMA íŒŒì¸íŠœë‹ ì‚¬ë¡€
    - **ë‚œì´ë„**: ë†’ìŒ | **ì ìš© ì‹œê°„**: 1ì£¼

#### **ìµœì‹  ë¦¬ë·° ë…¼ë¬¸**:

- **"Deep Learning for Financial Forecasting: A Review of Recent Advancements"** (2025)[^33]
    - LSTM, GRU, Transformer, Autoencoder ë¹„êµ
    - Scopus ë°ì´í„°ë² ì´ìŠ¤ 2020~2024ë…„ ë¶„ì„
- **"Time Series Forecasting with Transformer Models and Application to Asset Management"** (2023)[^34][^35]
    - Amundi ë¦¬ì„œì¹˜ì„¼í„° (ìì‚°ê´€ë¦¬ì‚¬ ê´€ì )
    - íŠ¸ë Œë“œ ì¶”ì¢…, ë³€ë™ì„± ì˜ˆì¸¡ ì‹¤ë¬´ ì‚¬ë¡€

***

## **â…¥. í•œêµ­ ê¸ˆìœµê¶Œ 2026 íˆ¬ì/ì‹¤í–‰ ì „ëµ**

### **6-1. ë‹¨ê³„ë³„ ì‹¤í–‰ ë¡œë“œë§µ**

**Phase 1 (1~3ì›”): ê²€ì¦ \& íŒŒì¼ëŸ¿**

- ì‹ ìš©í‰ê°€ AI: ê¸°ì—… ì¬ë¬´ë°ì´í„° ê¸°ë°˜ ë¶€ì‹¤í™” ì˜ˆì¸¡ ëª¨ë¸ (ì •ì±…ê¸ˆìœµê¸°ê´€ íŒ¨í‚¤ì§€ ê¸ˆìœµ ì—°ë™)
- ì´ìƒê±°ë˜íƒì§€: ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ FDS ê³ ë„í™” (ì¹´ì¹´ì˜¤ë±…í¬ ì‚¬ë¡€)
- ìƒì„±AI ìƒë‹´: AI ì—ì´ì „íŠ¸ ì²« ë°°í¬ (ì œí•œëœ ë„ë©”ì¸)

**Phase 2 (4~6ì›”): ê·œì œ ìƒŒë“œë°•ìŠ¤ + ë§ˆì´ë°ì´í„° 2.0**

- ë§ˆì´ë°ì´í„° 2.0 ê°œì¸ë§ì¶¤ ìì‚°ê´€ë¦¬ ì„œë¹„ìŠ¤ ë¡ ì¹­
- ì›í™” ìŠ¤í…Œì´ë¸”ì½”ì¸ PoC ê³ ë„í™”
- ë””ì§€í„¸ìì‚° ê±°ë˜ ì¸í”„ë¼ êµ¬ì¶•

**Phase 3 (7~9ì›”): ìœ ë™ì„± í€ë”ë©˜íƒˆ ìˆ˜í˜œ**

- 401k ì•”í˜¸í™”í ê°œë°© ìˆ˜í˜œ (êµ¬ì¡°ì  ì›”ê°„ ìë™ ìˆ˜ìš”)
- ê¸ˆë¦¬ ì¸í•˜ íš¨ê³¼ ë³¸ê²©í™” â†’ ìœ„í—˜ìì‚° ë ë¦¬
- AI ê¸°ë°˜ í¬íŠ¸í´ë¦¬ì˜¤ ìë™ ë¦¬ë°¸ëŸ°ì‹±

**Phase 4 (10~12ì›”): ì™„ì „ ìë™í™”**

- AI ì—ì´ì „íŠ¸ ì „ì‚¬í™” (ì˜ì‚¬ê²°ì • ìë™í™”)
- ìƒì„±AI ê¸°ë°˜ ë‚´ë¶€ ì—…ë¬´ ìë™í™” 30% ë‹¬ì„±
- ê¸ˆìœµê¶Œ AI í”Œë«í¼ ê²°ê³¼ë¬¼ í™œìš© ê·¹ëŒ€í™”

***

### **6-2. ê¸°ìˆ  íˆ¬ì ìš°ì„ ìˆœìœ„**

| ìš°ì„ ìˆœìœ„ | ê¸°ìˆ  | ê¸°ëŒ€ íš¨ê³¼ | íˆ¬ì ê·œëª¨ |
| :-- | :-- | :-- | :-- |
| **1ìˆœìœ„** | AI ì—ì´ì „íŠ¸ (ìƒë‹´/ì˜ì‚¬ê²°ì •) | ë¹„ìš© 50% ì ˆê°, ê³ ê° ë§Œì¡±ë„â†‘ | ì¤‘ìƒ |
| **2ìˆœìœ„** | í™•ë¥ ë¶„í¬ ê¸°ë°˜ ìœ„í—˜ê´€ë¦¬ | ë¦¬ìŠ¤í¬ ì •ëŸ‰í™”, ê·œì œ ì¤€ìˆ˜ | ì¤‘ |
| **3ìˆœìœ„** | ìƒì„±AI ìì²´ ëª¨ë¸ë§ | ë°ì´í„° ë³´ì•ˆ, ê¸ˆìœµíŠ¹í™” ê¸°ëŠ¥ | ìƒ |
| **4ìˆœìœ„** | ë””ì§€í„¸ìì‚° ê±°ë˜ ì‹œìŠ¤í…œ | ìƒˆë¡œìš´ ìˆ˜ìµì› ì°½ì¶œ | ì¤‘ìƒ |
| **5ìˆœìœ„** | ë§ˆì´ë°ì´í„° ê¸°ë°˜ AI ë¶„ì„ | ê³ ê° ë°ì´í„° í™œìš© ê·¹ëŒ€í™” | ì¤‘ |


***

## **â…¦. ì•Œê³ ë¦¬ì¦˜ ì‹¬í™”: ì‹¤ìŠµ ì‚¬ë¡€**

### **7-1. N-BEATSë¡œ ë¹„íŠ¸ì½”ì¸ ì˜ˆì¸¡í•˜ê¸°**

```python
# 1. ë°ì´í„° ì¤€ë¹„
import pyupbit
from neuralforecast.models import NBEATS
from neuralforecast.utils import train_test_split
import pandas as pd

df = pyupbit.get_ohlcv("KRW-BTC", interval="day", count=500)
df = df.reset_index().rename(columns={'index': 'ds', 'close': 'y'})
df['ds'] = pd.to_datetime(df['ds'])
df = df[['ds', 'y']].sort_values('ds')

# 2. ë°ì´í„° ë¶„í• 
Y_train_df, Y_test_df = train_test_split(df, test_size=0.2)

# 3. ëª¨ë¸ í›ˆë ¨
model = NBEATS(
    h=30,  # 30ì¼ ë¯¸ë˜ ì˜ˆì¸¡
    input_size=336,  # 336ì¼ ê³¼ê±° ì‚¬ìš©
    max_steps=100,
    batch_size=32
)
model.fit(Y_train_df, valid_df=Y_test_df)

# 4. ì˜ˆì¸¡
forecast_df = model.predict(Y_test_df)
```


***

### **7-2. TFTë¡œ ì „ê¸°ì†Œë¹„ ë‹¤ì¤‘ì§€í‰ ì˜ˆì¸¡**

```python
from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet
import pytorch_lightning as pl

# ë°ì´í„° ì¤€ë¹„ (ì‹œê°„, ì „ë ¥ì†Œë¹„, ê¸°ì˜¨, íœ´ì¼ ë“±)
training = TimeSeriesDataSet(
    df[df.date < cutoff_date],
    time_idx='date',
    target='power_consumption',
    group_ids=['customer_id'],
    max_encoder_length=336,  # 14ì¼ Ã— 24ì‹œê°„
    max_prediction_length=24,  # ë‚´ì¼ 24ì‹œê°„
    time_varying_known_reals=['hour', 'day_of_week', 'temperature'],
    time_varying_unknown_reals=['power_consumption'],
    static_categorical_features=['region_id']
)

# ëª¨ë¸ ì •ì˜
model = TemporalFusionTransformer.from_dataset(
    training_dataset=training,
    learning_rate=0.01,
    hidden_size=16,
    attention_head_size=4,
    dropout=0.1,
    hidden_continuous_size=8,
    output_size=24,  # ë‹¤ì¤‘ì§€í‰
    loss='quantile'  # ë¶„ìœ„ìˆ˜ ì†ì‹¤
)

# í›ˆë ¨
trainer = pl.Trainer(max_epochs=50, gpus=1 if torch.cuda.is_available() else 0)
trainer.fit(model, train_dataloader, val_dataloader)
```


***

### **7-3. ë¶„ìœ„ìˆ˜ íšŒê·€ë¡œ ì‹ ë¢° êµ¬ê°„ í™•ë³´**

```python
import tensorflow as tf
from tensorflow.keras.layers import Dense, LSTM, Input
from tensorflow.keras.models import Model

def quantile_loss(q, y_true, y_pred):
    """ë¶„ìœ„ìˆ˜ ì†ì‹¤í•¨ìˆ˜"""
    error = y_true - y_pred
    return tf.reduce_mean(
        tf.where(error >= 0, q * error, (q - 1) * error)
    )

# ë©€í‹°íƒœìŠ¤í¬: 3ê°€ì§€ ë¶„ìœ„ìˆ˜ ë™ì‹œ ì˜ˆì¸¡ (10th, 50th, 90th)
inputs = Input(shape=(lookback_window, n_features))
x = LSTM(64, return_sequences=True)(inputs)
x = LSTM(32)(x)

# 3ê°œì˜ ì¶œë ¥ í—¤ë“œ
q10 = Dense(1, name='q10')(x)
q50 = Dense(1, name='q50')(x)
q90 = Dense(1, name='q90')(x)

model = Model(inputs=inputs, outputs=[q10, q50, q90])

# ì»´íŒŒì¼ (ë¶„ìœ„ìˆ˜ë³„ ì†ì‹¤)
model.compile(
    optimizer='adam',
    loss=[
        lambda y_true, y_pred: quantile_loss(0.1, y_true, y_pred),
        lambda y_true, y_pred: quantile_loss(0.5, y_true, y_pred),
        lambda y_true, y_pred: quantile_loss(0.9, y_true, y_pred)
    ]
)

model.fit(X_train, [y_train, y_train, y_train], epochs=50, batch_size=32)

# ì˜ˆì¸¡
q10_pred, q50_pred, q90_pred = model.predict(X_test)
# ì‹ ë¢°ë„ 80% êµ¬ê°„: [q10_pred, q90_pred]
```


***

## **â…§. ì£¼ìš” ì œì•½ \& ê·¹ë³µ ì „ëµ**

### **8-1. N-BEATS ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­**

| ë¬¸ì œ | ì›ì¸ | ê·¹ë³µ ë°©ë²• |
| :-- | :-- | :-- |
| ê·¹ë„ ë³€ë™ì„±ì— ì•½í•¨ | ì‹œê³„ì—´ smoothness ê°€ì • | Quantile loss + ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ì˜ˆì¸¡ |
| ì™¸ìƒ ë³€ìˆ˜ ë°˜ì˜ ë¯¸í¡ | ê¸°ë³¸ N-BEATS ë‹¨ë³€ëŸ‰ ëª¨ë¸ | NBEATSx ì‚¬ìš© (ì™¸ìƒ ë³€ìˆ˜ í¬í•¨) |
| ì¥ê¸° ì˜ˆì¸¡ ì„±ëŠ¥ ì €í•˜ | ì˜¤í† ë ˆê·¸ë ˆì‹œë¸Œ ëˆ„ì  ì˜¤ì°¨ | ë‹¤ì¤‘ ìŠ¤í… í•™ìŠµ (multi-step training) |

### **8-2. TFTì˜ ë³µì¡ì„± ë° ë°ì´í„° ìš”êµ¬ì‚¬í•­**

- **ìµœì†Œ ë°ì´í„°**: 1,000+ ê´€ì¸¡ì¹˜ per ì‹œê³„ì—´
- **ê³„ì‚°ëŸ‰**: GPU ë©”ëª¨ë¦¬ 8GB ì´ìƒ ê¶Œì¥
- **ê·¹ë³µ**: ì‘ì€ ë°ì´í„°ì…‹ â†’ Prophet, ì¤‘ê°„ â†’ N-BEATS, ëŒ€ê·œëª¨ â†’ TFT


### **8-3. LLM ê¸°ë°˜ ì‹œê³„ì—´ ëª¨ë¸ì˜ í¸í–¥**

- **ë¬¸ì œ**: TimeGPT zero-shotì´ "í‰ê·  íšŒê·€" ê²½í–¥[^24]
- **ê·¹ë³µ**: Fine-tuning on domain-specific data, ensemble with classical models

***

## **â…¨. 2026ë…„ 10ëŒ€ AIÂ·ê¸ˆìœµ ë‰´ìŠ¤ ì´ìŠˆ**

1. **AI ì—ì´ì „íŠ¸ ì€í–‰ ë³¸ê²©í™”** â†’ ì§ì› ìˆ˜ 10~15% ê°ì†Œ ì‹œì‘
2. **ìŠ¤í…Œì´ë¸”ì½”ì¸ ê·œì œ ì™„í™”** â†’ ê¸€ë¡œë²Œ ê²°ì œ ë„¤íŠ¸ì›Œí¬ ê²½ìŸ ì‹¬í™”
3. **MoE ëª¨ë¸ ì‚°ì—… í‘œì¤€í™”** â†’ ê°œë³„ ê¸ˆìœµì‚¬ì˜ ì»¤ìŠ¤í…€ LLM êµ¬ì¶• ê°€ì†
4. **401k ì•”í˜¸í™”í ê°œë°©** (ë¯¸êµ­) â†’ ê¸°ê´€ ìê¸ˆ ìœ ì… ê·¹ëŒ€í™”
5. **ë§ˆì´ë°ì´í„° 2.0 ìˆ˜ìµí™”** (í•œêµ­) â†’ ê°œì¸ë§ì¶¤ ê¸ˆìœµìƒë‹´ AI í™•ì‚°
6. **DeepSeek-R1 ê¸°ë°˜ ì˜¤í”ˆì†ŒìŠ¤ ê¸ˆìœµ ëª¨ë¸** â†’ ëŒ€í˜• ì€í–‰ AI ìˆ˜ì… ë¹„ìš© ê¸‰ë½
7. **ê³µë§¤ë„ ì˜ˆì¸¡ AI ê·œì œ** â†’ ê³µì •í•œ ì‹œì¥ ê°ì‹œ ê¸°ìˆ  ê³ ë„í™”
8. **ìƒì„±AI ê·œì œ ì²´ê³„í™”** â†’ ê°ë…ë‹¹êµ­ AI ê°ì‹œ ì—­ëŸ‰ ê°•í™”
9. **ì–‘ì ê¸´ì¶• ì¢…ë£Œ â†’ ìœ ë™ì„± í™•ì¥** (ê¸€ë¡œë²Œ) â†’ ìœ„í—˜ìì‚° ì„ í˜¸ ì‹¬í™”
10. **ê¸ˆë¦¬ ì¸í•˜ ë³¸ê²©í™”** â†’ ì €ê¸ˆë¦¬ ì¡°ë‹¬ ê¸°ë°˜ ì‹ ê·œ ê¸ˆìœµìƒí’ˆ ë¼ì‹œ

***

## **â…©. ìµœì¢… ê¶Œê³ ì‚¬í•­ (AI ì „ê³µì ëŒ€ìƒ)**

### **ì¦‰ì‹œ ì‹¤í–‰ (1~3ê°œì›”)**

1. **NeuralForecast + Upbit ë°ì´í„°**ë¡œ ë¹„íŠ¸ì½”ì¸ ì˜ˆì¸¡ PoC êµ¬ì¶•
2. **Hugging Face Time Series Transformer** ëª¨ë¸ íŒŒì¸íŠœë‹
3. **PyMC/Pyro**ë¡œ ë² ì´ì§€ì•ˆ í™•ë¥ ë¶„í¬ ëª¨ë¸ 1ê°œ ì™„ì„±

### **ì¤‘ê¸° (3~6ê°œì›”)**

1. ê¸ˆìœµ ì‹œë®¬ë ˆì´ì…˜: êµ­ë‚´ ì£¼ê°€ 3ê°œì›” ì˜ˆì¸¡ ëª¨ë¸
2. ë…¼ë¬¸ êµ¬í˜„: "Quantile Deep Learning" ë¹„íŠ¸ì½”ì¸ ì ìš©
3. ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬: NeuralForecast/PyTorch Forecasting ë²„ê·¸ í”½ìŠ¤

### **ì¥ê¸° (6~12ê°œì›”)**

1. ìì²´ ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸: í•œêµ­ ë¶€ë„ìœ¨ ì˜ˆì¸¡ (LSTM + ì •í˜• ë°ì´í„°)
2. ë…¼ë¬¸ ì‘ì„±: "N-BEATS vs TFT on Korean Financial Data"
3. ìŠ¤íƒ€íŠ¸ì—…/ì§ë¬´ ì´ë™: Fintech, QuantFund, ëŒ€í˜• ì€í–‰ AI íŒ€

***

## **ì°¸ê³  ìë£Œ (Top References)**

| íƒ€ì… | ì œëª© | ì¶œì²˜ | ë‚œì´ë„ |
| :-- | :-- | :-- | :-- |
| **ë…¼ë¬¸** | N-BEATS | arXiv:1905.10437 | â­â­â­ |
| **ë…¼ë¬¸** | TFT | arXiv:1912.09363 | â­â­â­â­ |
| **ë…¼ë¬¸** | Quantile DL | arXiv:2411.15674 | â­â­â­ |
| **ë…¼ë¬¸** | TimeGPT-1 | arXiv:2310.03589 | â­â­â­ |
| **ë¼ì´ë¸ŒëŸ¬ë¦¬** | NeuralForecast | https://github.com/Nixtla | â­â­ |
| **ë¼ì´ë¸ŒëŸ¬ë¦¬** | PyTorch Forecasting | https://github.com/sktime | â­â­â­ |
| **ê¸°ì‚¬** | 2026 ê¸ˆìœµ AI ì „ë§ | Samsung SDS Insight | â­â­ |
| **ê¸°ì‚¬** | í•œêµ­ì€í–‰ AI ì •ì±… | ê¸ˆìœµìœ„ì›íšŒ | â­â­ |
| **íŠœí† ë¦¬ì–¼** | Time Series w/ HF | Hugging Face Blog | â­â­ |
| **íŠœí† ë¦¬ì–¼** | TimeGPT Getting Started | Nixtla ê³µì‹ | â­â­ |


***

## **Conclusion**

2026ë…„ì€ ê¸ˆìœµ AIê°€ **"ì‹¤í—˜ì‹¤ì—ì„œ ìš´ì˜ì‹¤ë¡œ"** ì´í–‰í•˜ëŠ” ë¶„ìˆ˜ë ¹ì´ë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ í™•ë¥ ë¶„í¬ ì˜ˆì¸¡ ëª¨ë¸ë“¤(N-BEATS, TFT, Quantile DL)ì€ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”ì˜ ì‚°ì—… í‘œì¤€ì´ ë  ê²ƒì´ê³ , ì¶”ë¡  ê°•í™” LLM(o3-pro, DeepSeek-R1)ì€ ê¸ˆìœµ ì˜ì‚¬ê²°ì •ì˜ ë³µì¡ì„±ì„ í•œ ë‹¨ê³„ í•´ì†Œí•  ê²ƒì´ë‹¤.

í•œêµ­ ê¸ˆìœµê¶Œì˜ AI ì—ì´ì „íŠ¸ ì „í™˜ì€ **ì •ì±… ì§€ì›(ë§ˆì´ë°ì´í„° 2.0, ê¸ˆìœµê¶Œ AI í”Œë«í¼) + ê¸€ë¡œë²Œ ìœ ë™ì„± í€ë”ë©˜íƒˆ(ì–‘ì ê¸´ì¶• ì¢…ë£Œ, 401k ì•”í˜¸í™”í ê°œë°©)** ì´ë¼ëŠ” ì™„ë²½í•œ í™˜ê²½ ì†ì—ì„œ ê°€ì†í™”ë  ê²ƒì´ë‹¤.

**AI ì „ê³µìë¼ë©´**: ì§€ê¸ˆ ì œì‹œí•œ ì˜¤í”ˆì†ŒìŠ¤ ìŠ¤íƒ(NeuralForecast, PyTorch Forecasting, Hugging Face)ê³¼ í•µì‹¬ ë…¼ë¬¸ë“¤ì„ ìµí˜€ **6ê°œì›” ë‚´ ìì²´ ê¸ˆìœµ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì„ ì™„ì„±**í•˜ê³ , ì´ë¥¼ **Fintech, QuantFund, ëŒ€í˜• ê¸ˆìœµì‚¬ AI íŒ€**ì˜ ì´ë ¥ì„œ í¬íŠ¸í´ë¦¬ì˜¤ë¡œ ì‚¼ìœ¼ë©´ 2026~2027ë…„ ê¸ˆìœµ AI ì‹œì¥ì˜ ì¤‘ì‹¬ ì¸ì¬ê°€ ë  ìˆ˜ ìˆë‹¤.

***

**Report Date**: 2026.02.02 (KST)
**Data Sources**: 155+ peer-reviewed papers, regulatory documents, industry reports (2024~2026)
**Next Update**: 2026.05 (ê¸ˆë¦¬ ì¸í•˜ ë³¸ê²©í™” ì´í›„)
<span style="display:none">[^100][^101][^102][^103][^104][^105][^106][^107][^108][^109][^110][^111][^112][^113][^114][^115][^116][^117][^118][^119][^120][^121][^36][^37][^38][^39][^40][^41][^42][^43][^44][^45][^46][^47][^48][^49][^50][^51][^52][^53][^54][^55][^56][^57][^58][^59][^60][^61][^62][^63][^64][^65][^66][^67][^68][^69][^70][^71][^72][^73][^74][^75][^76][^77][^78][^79][^80][^81][^82][^83][^84][^85][^86][^87][^88][^89][^90][^91][^92][^93][^94][^95][^96][^97][^98][^99]</span>

<div align="center">â‚</div>

[^1]: https://highendaily.com/55/?bmode=view\&idx=168578189

[^2]: https://magazine.hankyung.com/money/article/202512181472c

[^3]: https://byline.network/2026/01/0102/

[^4]: https://brunch.co.kr/@wineservice/367

[^5]: https://www.samsungsds.com/kr/insights/ai-in-banking-in-2025.html

[^6]: https://www.semanticscholar.org/paper/13c185b8c461034af2634f25dd8a85889e8ee135

[^7]: https://arxiv.org/pdf/1905.10437.pdf

[^8]: https://arxiv.org/pdf/2104.05522.pdf

[^9]: https://pubmed.ncbi.nlm.nih.gov/39240737/

[^10]: https://ieeexplore.ieee.org/document/9380649/

[^11]: https://onepetro.org/SJ/article/30/10/6236/787902/Exploring-the-Power-of-Neural-Basis-Expansion

[^12]: https://www.nature.com/articles/s41598-022-26499-y

[^13]: https://www.e-journal.uum.edu.my/index.php/jict/article/view/20874

[^14]: https://arxiv.org/abs/1912.09363

[^15]: https://www.mathworks.com/help/deeplearning/ug/time-series-forecasting-using-temporal-fusion-transformer.html

[^16]: https://github.com/sktime/pytorch-forecasting

[^17]: https://arxiv.org/abs/2411.15674

[^18]: https://arxiv.org/html/2411.15674v1

[^19]: https://arxiv.org/abs/2411.13921

[^20]: https://arxiv.org/html/2411.13921v2

[^21]: https://www.mdpi.com/2079-9292/14/6/1070

[^22]: https://arxiv.org/html/2509.23678v1

[^23]: https://www.lgcns.com/kr/moa/insight/detail.78

[^24]: http://arxiv.org/pdf/2310.03589.pdf

[^25]: https://www.datacamp.com/tutorial/time-series-forecasting-with-time-gpt

[^26]: https://github.com/Nixtla/neuralforecast

[^27]: https://nixtlaverse.nixtla.io/neuralforecast/docs/getting-started/introduction.html

[^28]: https://pytorch-forecasting.readthedocs.io/en/stable/getting-started.html

[^29]: https://huggingface.co/docs/transformers/en/model_doc/time_series_transformer

[^30]: https://www.griddynamics.com/blog/probabilistic-forecasting-demand-prediction

[^31]: https://arxiv.org/pdf/2310.07820.pdf

[^32]: http://arxiv.org/pdf/2306.11025v1.pdf

[^33]: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5263710

[^34]: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4375798

[^35]: https://research-center.amundi.com/article/time-series-forecasting-transformer-models-and-application-asset-management

[^36]: https://ieeexplore.ieee.org/document/11034281/

[^37]: https://www.mdpi.com/2079-9292/14/7/1266

[^38]: https://link.springer.com/10.1007/s00477-025-03100-2

[^39]: https://link.springer.com/10.1186/s12879-025-12440-x

[^40]: https://www.technoskypub.com/journals/acm-2025-080405/

[^41]: https://www.mdpi.com/1099-4300/28/2/133

[^42]: https://arxiv.org/abs/2503.23102

[^43]: https://ritha.eu/journals/JAES/issues/87/articles/2

[^44]: https://ieeexplore.ieee.org/document/11182129/

[^45]: https://www.jcdr.net/article_fulltext.asp?issn=0973-709x\&year=2026\&volume=20\&issue=1\&page=OC01\&issn=0973-709x\&id=22198

[^46]: https://arxiv.org/pdf/2112.02905.pdf

[^47]: http://arxiv.org/pdf/2211.14730v2.pdf

[^48]: https://arxiv.org/pdf/2405.13810.pdf

[^49]: https://arxiv.org/pdf/2202.01381.pdf

[^50]: https://arxiv.org/pdf/2304.08424.pdf

[^51]: https://arxiv.org/pdf/2310.20218.pdf

[^52]: http://arxiv.org/pdf/2410.23749.pdf

[^53]: http://arxiv.org/pdf/2310.06625.pdf

[^54]: https://www.npcelectric.com/news/transformer-market-2025-performance-and-2026-outlook.html

[^55]: https://www.mordorintelligence.com/industry-reports/united-states-distribution-transformer-market

[^56]: https://journalwjaets.com/sites/default/files/fulltext_pdf/WJAETS-2025-0167.pdf

[^57]: https://arxiv.org/html/2310.01232v2

[^58]: https://www.polarismarketresearch.com/industry-analysis/distribution-transformer-market

[^59]: https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2026.1722121/full

[^60]: https://www.precedenceresearch.com/distribution-transformer-market

[^61]: https://dl.acm.org/doi/10.1145/3785706.3785719

[^62]: https://arxiv.org/abs/2403.02523

[^63]: https://www.woodmac.com/press-releases/power-transformers-and-distribution-transformers-will-face-supply-deficits-of-30-and-10-in-2025/

[^64]: https://www.imf.org/en/publications/wp/issues/2026/01/30/nowcasting-economic-growth-with-machine-learning-and-satellite-data-573623

[^65]: https://blogs.mathworks.com/finance/2024/02/02/deep-learning-in-quantitative-finance-transformer-networks-for-time-series-prediction/

[^66]: https://ace.ewapublishing.org/media/23ac2c18aa8f4680ab196d6d9b8d2d86.marked.pdf

[^67]: https://ijbms.net/assets/files/1728821347.pdf

[^68]: http://arxiv.org/pdf/2410.15951.pdf

[^69]: https://arxiv.org/pdf/2411.13562.pdf

[^70]: https://ijcsrr.org/wp-content/uploads/2024/01/07-0501-2024.pdf

[^71]: https://www.ijfmr.com/papers/2024/5/29059.pdf

[^72]: https://ace.ewapublishing.org/media/7b34f29f569b42a4a860c95856bed70a.marked.pdf

[^73]: https://ijsra.net/sites/default/files/IJSRA-2024-0639.pdf

[^74]: https://blog.naver.com/PostView.naver?blogId=rainbowjini\&logNo=223696078627

[^75]: https://paulsmedia.tistory.com/entry/ë¹„íŠ¸ì½”ì¸-ì£¼ê°€-ì˜ˆì¸¡ì„-ìœ„í•œ-ë¨¸ì‹ ëŸ¬ë‹-ê¸°ìˆ -í™œìš©-ë°©ë²•-ë‘-ë²ˆì§¸-ì´ì•¼ê¸°-70

[^76]: https://brunch.co.kr/@kid008/584

[^77]: https://eiec.kdi.re.kr/policy/domesticView.do?ac=0000202325

[^78]: https://datacook.tistory.com/64

[^79]: https://www.cio.com/article/4111617/ìƒì„±í˜•-aiê°€-it-ì „ëµì„-ë°”ê¾¼ë‹¤-2026-it-ì „ë§-ì¡°ì‚¬-ê²°ê³¼.html

[^80]: https://www.manuscriptlink.com/society/kips/conference/ask2022/file/downloadSoConfManuscript/abs/KIPS_C2022A0020

[^81]: https://contents.premium.naver.com/busymoon/kicpakpmg/contents/260131114416841kz

[^82]: https://www.youtube.com/watch?v=hliEzB_ToTg

[^83]: https://chunws13.tistory.com/66

[^84]: https://www.hani.co.kr/arti/economy/economy_general/1238152.html

[^85]: https://ieeexplore.ieee.org/document/11237118/

[^86]: https://ieeexplore.ieee.org/document/10849645/

[^87]: https://link.springer.com/10.1007/978-3-031-72347-6_17

[^88]: https://linkinghub.elsevier.com/retrieve/pii/S0169207022000413

[^89]: http://arxiv.org/pdf/2307.09797.pdf

[^90]: https://arxiv.org/pdf/2302.02597.pdf

[^91]: https://arxiv.org/pdf/2102.00397.pdf

[^92]: http://arxiv.org/pdf/2312.15002.pdf

[^93]: http://arxiv.org/pdf/2404.03737.pdf

[^94]: https://github.com/someonetookmynugget/Time-Series-Forecasting

[^95]: https://nixtlaverse.nixtla.io/neuralforecast/models.nbeats.html

[^96]: https://openreview.net/pdf?id=r1ecqn4YwB

[^97]: https://www.kcl.ac.uk/business/assets/pdf/dafm-working-papers/2021-papers/deep-quantile-regression.pdf

[^98]: https://towardsdatascience.com/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538/

[^99]: https://www.youtube.com/watch?v=V14qoa5vZ1I

[^100]: https://academic.oup.com/jfec/article/22/3/636/7163191

[^101]: https://arxiv.org/abs/1905.10437

[^102]: https://aihorizonforecast.substack.com/p/temporal-fusion-transformer-time

[^103]: https://www.nature.com/articles/s41598-021-90063-3

[^104]: https://psyjournals.ru/en/journals/mda/archive/2024_n2/Svekolnikova_Panovskiy

[^105]: https://arxiv.org/abs/2402.12694

[^106]: https://piqm.saharadigitals.com/volume/2/issue/1/article/67

[^107]: https://www.mdpi.com/2673-4591/18/1/30

[^108]: https://arxiv.org/abs/2509.10560

[^109]: https://www.nature.com/articles/s41598-025-30874-w

[^110]: https://www.frontiersin.org/articles/10.3389/fdata.2025.1745751/full

[^111]: https://www.mdpi.com/2813-0324/11/1/32

[^112]: https://ieeexplore.ieee.org/document/11336525/

[^113]: https://arxiv.org/abs/2508.18635

[^114]: https://arxiv.org/pdf/2402.16516.pdf

[^115]: https://arxiv.org/pdf/2310.10688.pdf

[^116]: http://arxiv.org/pdf/2405.14252.pdf

[^117]: http://arxiv.org/pdf/2310.04948.pdf

[^118]: https://arxiv.org/pdf/2403.05798.pdf

[^119]: https://maucher.pages.mi.hdm-stuttgart.de/mlimpsequential/huggingFaceTimeSeries.html

[^120]: https://github.com/Abdulmajid-Murad/deep_probabilistic_forecast

[^121]: https://github.com/Nixtla/nixtla

